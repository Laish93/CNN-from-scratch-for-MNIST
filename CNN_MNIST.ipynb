{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77358570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mnist import MNIST\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d02ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding MNIST dataset\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4abb9f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe22a0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data visualization\n",
    "num_classes = 10\n",
    "\n",
    "f, ax = plt.subplots(1, num_classes, figsize = (20,20))\n",
    "\n",
    "for i in range(0, num_classes):\n",
    "    sample = train_images[train_labels == i][0]\n",
    "    ax[i].imshow(sample, cmap='gray')\n",
    "    ax[i].set_title(f'Label: {i}', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd32a67",
   "metadata": {},
   "source": [
    "### Define Convolution layer + ReLU activation and its backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c5b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(output):\n",
    "    '''\n",
    "    ReLU activation function\n",
    "    '''\n",
    "    output[output<=0] = 0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4120ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "# Defining a Convolution layer by using 3x3 filters.\n",
    "\n",
    "    def __init__(self, num_of_filters):\n",
    "        self.num_of_filters = num_of_filters\n",
    "\n",
    "        # filters => (num_filters, 3, 3)\n",
    "        self.filters = np.random.randn(num_of_filters, 3, 3) / 9\n",
    "\n",
    "    def generator(self, image):\n",
    "        '''\n",
    "        Generator function allows to declare a function that behaves like an iterator\n",
    "        No padding was used since pytorch.conv2d function's default is also padding = 0\n",
    "        '''\n",
    "        height, width = image.shape\n",
    "\n",
    "        for i in range(height - 2):\n",
    "            for j in range(width - 2):\n",
    "                image_region = image[i:(i + 3), j:(j + 3)]\n",
    "                yield image_region, i, j\n",
    "    \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Forward pass of the convolution layer\n",
    "        '''\n",
    "        self.final_input = input\n",
    "\n",
    "        height, width = input.shape\n",
    "        # create following matrix for store values of the convolution operation\n",
    "        output = np.zeros((height - 2, width - 2, self.num_of_filters))\n",
    "\n",
    "        for image_region, i, j in self.generator(input):\n",
    "            # multiply elements of two arrays and get 3d array\n",
    "            # numpy sum fucntion is used to convert it to 1s array which is same length as num_of_filters\n",
    "            output[i, j] = np.sum(image_region * self.filters, axis=(1, 2))\n",
    "                    \n",
    "        # output gives the result of  convolution + ReLU process\n",
    "        return output\n",
    "\n",
    "    def backpropagation(self, dL_dout, learning_rate):\n",
    "        '''\n",
    "        Backward pass of the convolution layer.\n",
    "        dL_dout is the loss gradient for this layer's outputs\n",
    "        '''\n",
    "        dL_dfilters = np.zeros(self.filters.shape)\n",
    "\n",
    "        for image_region, i, j in self.generator(self.final_input):\n",
    "            for f in range(self.num_of_filters):\n",
    "                dL_dfilters[f] += dL_dout[i, j, f] * image_region\n",
    "\n",
    "        # Update filters by using Stochastic Gradient Descent\n",
    "        self.filters -= learning_rate * dL_dfilters\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40fda4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 26, 32)\n"
     ]
    }
   ],
   "source": [
    "# Checking the convolutional layer\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "\n",
    "convolution = Convolution(32)\n",
    "output = convolution.forward(train_images[0])\n",
    "\n",
    "# Output is (26,26,32) because padding was not used\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906845d4",
   "metadata": {},
   "source": [
    "### Define Maxpool layer and its backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d17ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2d:\n",
    "  # A Max Pooling layer using a pool size of 2.\n",
    "\n",
    "    def generator(self, image):\n",
    "        '''\n",
    "        This generates non-overlapping 2x2 image regions to pool over\n",
    "        '''\n",
    "        height, width, _ = image.shape\n",
    "        new_height = height // 2\n",
    "        new_width = width // 2\n",
    "\n",
    "        for i in range(new_height):\n",
    "            for j in range(new_width):\n",
    "                image_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "                yield image_region, i, j\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Forward pass of the maxpool layer\n",
    "        '''\n",
    "        self.last_input = input\n",
    "\n",
    "        height, width, num_of_filters = input.shape\n",
    "        # creating a matrix to  to keep the values of maxpool operation\n",
    "        output = np.zeros((height // 2, width // 2, num_of_filters))\n",
    "\n",
    "        for image_region, i, j in self.generator(input):\n",
    "            # each step, numpy.amax returns the maximum value within the window\n",
    "            output[i, j] = np.amax(image_region, axis=(0, 1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backpropagation(self, dL_dout):\n",
    "        '''\n",
    "        Backward pass of the maxpool layer.\n",
    "        Output is the loss gradient for this layer's inputs.\n",
    "        dL_dout is the loss gradient for this layer's outputs.\n",
    "        '''\n",
    "        dL_dinput = np.zeros(self.last_input.shape)\n",
    "\n",
    "        for image_region, i, j in self.generator(self.last_input):\n",
    "            height, width, f = image_region.shape\n",
    "            # np.amax return the maximum of an array or maximum along an axis\n",
    "            amax = np.amax(image_region, axis=(0, 1))\n",
    "\n",
    "        for i2 in range(height):\n",
    "            for j2 in range(width):\n",
    "                for f2 in range(f):\n",
    "                    # Copying gradient if this pixel is max value\n",
    "                    if image_region[i2, j2, f2] == amax[f2]:\n",
    "                        dL_dinput[i * 2 + i2, j * 2 + j2, f2] = dL_dout[i, j, f2]\n",
    "\n",
    "        return dL_dinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5dee646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 13, 32)\n"
     ]
    }
   ],
   "source": [
    "# Checking convolutional layer + ReLU + maxpool layer\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "\n",
    "convolution = Convolution(32)\n",
    "maxpool = MaxPool2d()\n",
    "\n",
    "output = convolution.forward(train_images[0])\n",
    "output = maxpool.forward(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1c6c7b",
   "metadata": {},
   "source": [
    "### Define fully connected layer with Softmax activation and its backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a722c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "  \n",
    "    def __init__(self, input_length, nodes):\n",
    "        self.weights = np.random.randn(input_length, nodes) / input_length\n",
    "        self.bias = np.zeros(nodes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Forward pass of the softmax layer\n",
    "        '''\n",
    "        self.final_input_shape = input.shape\n",
    "\n",
    "        input = input.flatten()\n",
    "        self.final_input = input\n",
    "\n",
    "        input_length, nodes = self.weights.shape\n",
    "        \n",
    "        # values passing to the softmax function\n",
    "        total = np.dot(input, self.weights) + self.bias\n",
    "        self.final_total = total\n",
    "\n",
    "        exp = np.exp(total)\n",
    "        return exp / np.sum(exp, axis = 0)\n",
    " \n",
    "    def backpropagation(self, dL_dout, learning_rate):\n",
    "        '''\n",
    "        Performs a backward pass of the softmax layer.\n",
    "        Output is the loss gradient for this layer's inputs.\n",
    "        dL_dout is the loss gradient for this layer's outputs.\n",
    "        '''\n",
    "        # Only 1 element of dL_dout will be nonzero\n",
    "        for i, gradient in enumerate(dL_dout):\n",
    "            if gradient == 0:\n",
    "                continue\n",
    "\n",
    "            # exp^(total)\n",
    "            expo_t = np.exp(self.final_total)\n",
    "\n",
    "            # Sum of all expo^(total)\n",
    "            S = np.sum(expo_t)\n",
    "\n",
    "            # Gradients of out[i] with respect to total\n",
    "            dout_dt = -expo_t[i] * expo_t / (S ** 2)\n",
    "            dout_dt[i] = expo_t[i] * (S - expo_t[i]) / (S ** 2)\n",
    "\n",
    "            # Gradients of total with respect to weights, bias and input\n",
    "            dt_dw = self.final_input\n",
    "            dt_db = 1\n",
    "            dt_dinput = self.weights\n",
    "\n",
    "            # Gradients of loss with respect to total\n",
    "            dL_dt = gradient * dout_dt\n",
    "\n",
    "            # Gradients of loss with respect to weights, bias and input\n",
    "            # np.newaxis function is used to create a new axis of length one\n",
    "            dL_dw = dt_dw[np.newaxis].T @ dL_dt[np.newaxis]\n",
    "            dL_db = dL_dt * dt_db\n",
    "            dL_dinput = dt_dinput @ dL_dt\n",
    "\n",
    "            # Update weights and bias using Stochastic Gradient Descent (SGD)\n",
    "            self.weights -= learning_rate * dL_dw\n",
    "            self.bias -= learning_rate * dL_db\n",
    "\n",
    "            # reshape the output because of the flattening in the forward layer\n",
    "            return dL_dinput.reshape(self.final_input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc9380",
   "metadata": {},
   "source": [
    "### Training the Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd5fe294",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_images = mnist.train_images()[:30000]\n",
    "train_labels = mnist.train_labels()[:30000]\n",
    "test_images = mnist.test_images()[:]\n",
    "test_labels = mnist.test_labels()[:]\n",
    "\n",
    "convolution = Convolution(32)\n",
    "maxpool = MaxPool2d()\n",
    "softmax = Softmax(13 * 13 * 32, 10)     \n",
    "\n",
    "def forward(image, label):\n",
    "    '''\n",
    "    Completes a forward pass of the CNN\n",
    "    '''\n",
    "    # Transform the image from [0, 255] to [-0.5, 0.5] to make it easier to work\n",
    "    out = convolution.forward((image / 255) - 0.5)\n",
    "    \n",
    "    # Pass through the ReLu activation\n",
    "    out[out<=0] = 0 \n",
    "    \n",
    "    out = maxpool.forward(out)\n",
    "    out = softmax.forward(out)\n",
    "\n",
    "    # Calculate cross entropy loss\n",
    "    loss = -np.log(out[label])\n",
    "    \n",
    "    # calculating the accuracy\n",
    "    accuracy = 1 if np.argmax(out) == label else 0\n",
    "\n",
    "    return out, loss, accuracy\n",
    "\n",
    "def train(im, label, learning_rate = 0.005):\n",
    "    '''\n",
    "    Completes a full training step on the image and label\n",
    "    Returns the loss and accuracy\n",
    "    '''\n",
    "    # Forward\n",
    "    out, loss, accuracy = forward(im, label)\n",
    "\n",
    "    # Calculate initial gradient\n",
    "    # output from the softmax which is vector of 10 probabilities\n",
    "    # optimizer.zero_grad\n",
    "    gradient = np.zeros(10)\n",
    "    gradient[label] = -1 / out[label]\n",
    "\n",
    "    # Backpropagation\n",
    "    gradient = softmax.backpropagation(gradient, learning_rate)\n",
    "    gradient = maxpool.backpropagation(gradient)\n",
    "    \n",
    "    # backpropagate through ReLU activation\n",
    "    gradient[gradient<=0] = 0\n",
    "    gradient = convolution.backpropagation(gradient, learning_rate)\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19fe51ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 dictionaries to store accuracy/epoch and loss/epoch for both train and validation datasets\n",
    "\n",
    "accuracy_stats = {\n",
    "    'train': []\n",
    "}\n",
    "loss_stats = {\n",
    "    'train': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75c78d17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Training !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1 ---\n",
      "[Step 100] Past 100 steps: Average Loss 2.236 | Accuracy: 20%\n",
      "[Step 200] Past 100 steps: Average Loss 2.199 | Accuracy: 25%\n",
      "[Step 300] Past 100 steps: Average Loss 1.985 | Accuracy: 32%\n",
      "[Step 400] Past 100 steps: Average Loss 1.909 | Accuracy: 46%\n",
      "[Step 500] Past 100 steps: Average Loss 1.853 | Accuracy: 51%\n",
      "[Step 600] Past 100 steps: Average Loss 1.604 | Accuracy: 62%\n",
      "[Step 700] Past 100 steps: Average Loss 1.759 | Accuracy: 48%\n",
      "[Step 800] Past 100 steps: Average Loss 1.621 | Accuracy: 60%\n",
      "[Step 900] Past 100 steps: Average Loss 1.610 | Accuracy: 62%\n",
      "[Step 1000] Past 100 steps: Average Loss 1.481 | Accuracy: 72%\n",
      "[Step 1100] Past 100 steps: Average Loss 1.343 | Accuracy: 73%\n",
      "[Step 1200] Past 100 steps: Average Loss 1.388 | Accuracy: 72%\n",
      "[Step 1300] Past 100 steps: Average Loss 1.303 | Accuracy: 72%\n",
      "[Step 1400] Past 100 steps: Average Loss 1.272 | Accuracy: 76%\n",
      "[Step 1500] Past 100 steps: Average Loss 1.145 | Accuracy: 81%\n",
      "[Step 1600] Past 100 steps: Average Loss 1.258 | Accuracy: 73%\n",
      "[Step 1700] Past 100 steps: Average Loss 1.125 | Accuracy: 78%\n",
      "[Step 1800] Past 100 steps: Average Loss 1.093 | Accuracy: 79%\n",
      "[Step 1900] Past 100 steps: Average Loss 1.168 | Accuracy: 74%\n",
      "[Step 2000] Past 100 steps: Average Loss 1.044 | Accuracy: 83%\n",
      "[Step 2100] Past 100 steps: Average Loss 0.968 | Accuracy: 84%\n",
      "[Step 2200] Past 100 steps: Average Loss 0.994 | Accuracy: 81%\n",
      "[Step 2300] Past 100 steps: Average Loss 1.119 | Accuracy: 72%\n",
      "[Step 2400] Past 100 steps: Average Loss 0.893 | Accuracy: 87%\n",
      "[Step 2500] Past 100 steps: Average Loss 0.824 | Accuracy: 93%\n",
      "[Step 2600] Past 100 steps: Average Loss 0.887 | Accuracy: 83%\n",
      "[Step 2700] Past 100 steps: Average Loss 0.881 | Accuracy: 79%\n",
      "[Step 2800] Past 100 steps: Average Loss 1.001 | Accuracy: 80%\n",
      "[Step 2900] Past 100 steps: Average Loss 0.912 | Accuracy: 84%\n",
      "[Step 3000] Past 100 steps: Average Loss 0.817 | Accuracy: 88%\n",
      "[Step 3100] Past 100 steps: Average Loss 0.871 | Accuracy: 85%\n",
      "[Step 3200] Past 100 steps: Average Loss 0.913 | Accuracy: 78%\n",
      "[Step 3300] Past 100 steps: Average Loss 0.866 | Accuracy: 79%\n",
      "[Step 3400] Past 100 steps: Average Loss 0.894 | Accuracy: 83%\n",
      "[Step 3500] Past 100 steps: Average Loss 0.990 | Accuracy: 74%\n",
      "[Step 3600] Past 100 steps: Average Loss 0.781 | Accuracy: 84%\n",
      "[Step 3700] Past 100 steps: Average Loss 0.830 | Accuracy: 87%\n",
      "[Step 3800] Past 100 steps: Average Loss 0.822 | Accuracy: 86%\n",
      "[Step 3900] Past 100 steps: Average Loss 0.741 | Accuracy: 93%\n",
      "[Step 4000] Past 100 steps: Average Loss 0.711 | Accuracy: 87%\n",
      "[Step 4100] Past 100 steps: Average Loss 0.740 | Accuracy: 86%\n",
      "[Step 4200] Past 100 steps: Average Loss 0.773 | Accuracy: 87%\n",
      "[Step 4300] Past 100 steps: Average Loss 0.707 | Accuracy: 91%\n",
      "[Step 4400] Past 100 steps: Average Loss 0.862 | Accuracy: 82%\n",
      "[Step 4500] Past 100 steps: Average Loss 0.832 | Accuracy: 78%\n",
      "[Step 4600] Past 100 steps: Average Loss 0.624 | Accuracy: 92%\n",
      "[Step 4700] Past 100 steps: Average Loss 0.689 | Accuracy: 90%\n",
      "[Step 4800] Past 100 steps: Average Loss 0.772 | Accuracy: 86%\n",
      "[Step 4900] Past 100 steps: Average Loss 0.760 | Accuracy: 84%\n",
      "[Step 5000] Past 100 steps: Average Loss 0.771 | Accuracy: 84%\n",
      "[Step 5100] Past 100 steps: Average Loss 0.778 | Accuracy: 83%\n",
      "[Step 5200] Past 100 steps: Average Loss 0.802 | Accuracy: 83%\n",
      "[Step 5300] Past 100 steps: Average Loss 0.739 | Accuracy: 89%\n",
      "[Step 5400] Past 100 steps: Average Loss 0.633 | Accuracy: 87%\n",
      "[Step 5500] Past 100 steps: Average Loss 0.691 | Accuracy: 87%\n",
      "[Step 5600] Past 100 steps: Average Loss 0.749 | Accuracy: 86%\n",
      "[Step 5700] Past 100 steps: Average Loss 0.748 | Accuracy: 84%\n",
      "[Step 5800] Past 100 steps: Average Loss 0.766 | Accuracy: 82%\n",
      "[Step 5900] Past 100 steps: Average Loss 0.609 | Accuracy: 90%\n",
      "[Step 6000] Past 100 steps: Average Loss 0.699 | Accuracy: 88%\n",
      "[Step 6100] Past 100 steps: Average Loss 0.677 | Accuracy: 85%\n",
      "[Step 6200] Past 100 steps: Average Loss 0.737 | Accuracy: 79%\n",
      "[Step 6300] Past 100 steps: Average Loss 0.729 | Accuracy: 87%\n",
      "[Step 6400] Past 100 steps: Average Loss 0.698 | Accuracy: 86%\n",
      "[Step 6500] Past 100 steps: Average Loss 0.778 | Accuracy: 82%\n",
      "[Step 6600] Past 100 steps: Average Loss 0.729 | Accuracy: 86%\n",
      "[Step 6700] Past 100 steps: Average Loss 0.620 | Accuracy: 86%\n",
      "[Step 6800] Past 100 steps: Average Loss 0.816 | Accuracy: 85%\n",
      "[Step 6900] Past 100 steps: Average Loss 0.614 | Accuracy: 93%\n",
      "[Step 7000] Past 100 steps: Average Loss 0.835 | Accuracy: 78%\n",
      "[Step 7100] Past 100 steps: Average Loss 0.692 | Accuracy: 83%\n",
      "[Step 7200] Past 100 steps: Average Loss 0.661 | Accuracy: 87%\n",
      "[Step 7300] Past 100 steps: Average Loss 0.602 | Accuracy: 88%\n",
      "[Step 7400] Past 100 steps: Average Loss 0.698 | Accuracy: 84%\n",
      "[Step 7500] Past 100 steps: Average Loss 0.592 | Accuracy: 88%\n",
      "[Step 7600] Past 100 steps: Average Loss 0.609 | Accuracy: 88%\n",
      "[Step 7700] Past 100 steps: Average Loss 0.641 | Accuracy: 84%\n",
      "[Step 7800] Past 100 steps: Average Loss 0.645 | Accuracy: 89%\n",
      "[Step 7900] Past 100 steps: Average Loss 0.652 | Accuracy: 87%\n",
      "[Step 8000] Past 100 steps: Average Loss 0.637 | Accuracy: 87%\n",
      "[Step 8100] Past 100 steps: Average Loss 0.676 | Accuracy: 80%\n",
      "[Step 8200] Past 100 steps: Average Loss 0.556 | Accuracy: 92%\n",
      "[Step 8300] Past 100 steps: Average Loss 0.568 | Accuracy: 82%\n",
      "[Step 8400] Past 100 steps: Average Loss 0.589 | Accuracy: 90%\n",
      "[Step 8500] Past 100 steps: Average Loss 0.670 | Accuracy: 84%\n",
      "[Step 8600] Past 100 steps: Average Loss 0.573 | Accuracy: 88%\n",
      "[Step 8700] Past 100 steps: Average Loss 0.642 | Accuracy: 87%\n",
      "[Step 8800] Past 100 steps: Average Loss 0.515 | Accuracy: 92%\n",
      "[Step 8900] Past 100 steps: Average Loss 0.611 | Accuracy: 88%\n",
      "[Step 9000] Past 100 steps: Average Loss 0.595 | Accuracy: 88%\n",
      "[Step 9100] Past 100 steps: Average Loss 0.556 | Accuracy: 93%\n",
      "[Step 9200] Past 100 steps: Average Loss 0.659 | Accuracy: 84%\n",
      "[Step 9300] Past 100 steps: Average Loss 0.577 | Accuracy: 88%\n",
      "[Step 9400] Past 100 steps: Average Loss 0.594 | Accuracy: 83%\n",
      "[Step 9500] Past 100 steps: Average Loss 0.588 | Accuracy: 88%\n",
      "[Step 9600] Past 100 steps: Average Loss 0.601 | Accuracy: 88%\n",
      "[Step 9700] Past 100 steps: Average Loss 0.594 | Accuracy: 84%\n",
      "[Step 9800] Past 100 steps: Average Loss 0.606 | Accuracy: 83%\n",
      "[Step 9900] Past 100 steps: Average Loss 0.557 | Accuracy: 92%\n",
      "[Step 10000] Past 100 steps: Average Loss 0.593 | Accuracy: 85%\n",
      "[Step 10100] Past 100 steps: Average Loss 0.595 | Accuracy: 83%\n",
      "[Step 10200] Past 100 steps: Average Loss 0.546 | Accuracy: 90%\n",
      "[Step 10300] Past 100 steps: Average Loss 0.549 | Accuracy: 88%\n",
      "[Step 10400] Past 100 steps: Average Loss 0.601 | Accuracy: 87%\n",
      "[Step 10500] Past 100 steps: Average Loss 0.458 | Accuracy: 91%\n",
      "[Step 10600] Past 100 steps: Average Loss 0.576 | Accuracy: 89%\n",
      "[Step 10700] Past 100 steps: Average Loss 0.437 | Accuracy: 93%\n",
      "[Step 10800] Past 100 steps: Average Loss 0.486 | Accuracy: 88%\n",
      "[Step 10900] Past 100 steps: Average Loss 0.430 | Accuracy: 93%\n",
      "[Step 11000] Past 100 steps: Average Loss 0.565 | Accuracy: 90%\n",
      "[Step 11100] Past 100 steps: Average Loss 0.560 | Accuracy: 90%\n",
      "[Step 11200] Past 100 steps: Average Loss 0.479 | Accuracy: 92%\n",
      "[Step 11300] Past 100 steps: Average Loss 0.448 | Accuracy: 93%\n",
      "[Step 11400] Past 100 steps: Average Loss 0.529 | Accuracy: 89%\n",
      "[Step 11500] Past 100 steps: Average Loss 0.558 | Accuracy: 85%\n",
      "[Step 11600] Past 100 steps: Average Loss 0.530 | Accuracy: 89%\n",
      "[Step 11700] Past 100 steps: Average Loss 0.443 | Accuracy: 90%\n",
      "[Step 11800] Past 100 steps: Average Loss 0.400 | Accuracy: 96%\n",
      "[Step 11900] Past 100 steps: Average Loss 0.457 | Accuracy: 90%\n",
      "[Step 12000] Past 100 steps: Average Loss 0.482 | Accuracy: 91%\n",
      "[Step 12100] Past 100 steps: Average Loss 0.563 | Accuracy: 87%\n",
      "[Step 12200] Past 100 steps: Average Loss 0.524 | Accuracy: 93%\n",
      "[Step 12300] Past 100 steps: Average Loss 0.607 | Accuracy: 88%\n",
      "[Step 12400] Past 100 steps: Average Loss 0.532 | Accuracy: 85%\n",
      "[Step 12500] Past 100 steps: Average Loss 0.546 | Accuracy: 87%\n",
      "[Step 12600] Past 100 steps: Average Loss 0.480 | Accuracy: 89%\n",
      "[Step 12700] Past 100 steps: Average Loss 0.621 | Accuracy: 83%\n",
      "[Step 12800] Past 100 steps: Average Loss 0.430 | Accuracy: 92%\n",
      "[Step 12900] Past 100 steps: Average Loss 0.470 | Accuracy: 90%\n",
      "[Step 13000] Past 100 steps: Average Loss 0.517 | Accuracy: 89%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 13100] Past 100 steps: Average Loss 0.475 | Accuracy: 91%\n",
      "[Step 13200] Past 100 steps: Average Loss 0.464 | Accuracy: 88%\n",
      "[Step 13300] Past 100 steps: Average Loss 0.552 | Accuracy: 88%\n",
      "[Step 13400] Past 100 steps: Average Loss 0.525 | Accuracy: 88%\n",
      "[Step 13500] Past 100 steps: Average Loss 0.508 | Accuracy: 90%\n",
      "[Step 13600] Past 100 steps: Average Loss 0.482 | Accuracy: 89%\n",
      "[Step 13700] Past 100 steps: Average Loss 0.445 | Accuracy: 92%\n",
      "[Step 13800] Past 100 steps: Average Loss 0.618 | Accuracy: 80%\n",
      "[Step 13900] Past 100 steps: Average Loss 0.495 | Accuracy: 87%\n",
      "[Step 14000] Past 100 steps: Average Loss 0.454 | Accuracy: 91%\n",
      "[Step 14100] Past 100 steps: Average Loss 0.456 | Accuracy: 90%\n",
      "[Step 14200] Past 100 steps: Average Loss 0.384 | Accuracy: 93%\n",
      "[Step 14300] Past 100 steps: Average Loss 0.449 | Accuracy: 88%\n",
      "[Step 14400] Past 100 steps: Average Loss 0.416 | Accuracy: 89%\n",
      "[Step 14500] Past 100 steps: Average Loss 0.502 | Accuracy: 84%\n",
      "[Step 14600] Past 100 steps: Average Loss 0.424 | Accuracy: 89%\n",
      "[Step 14700] Past 100 steps: Average Loss 0.435 | Accuracy: 90%\n",
      "[Step 14800] Past 100 steps: Average Loss 0.471 | Accuracy: 91%\n",
      "[Step 14900] Past 100 steps: Average Loss 0.613 | Accuracy: 86%\n",
      "[Step 15000] Past 100 steps: Average Loss 0.514 | Accuracy: 86%\n",
      "[Step 15100] Past 100 steps: Average Loss 0.412 | Accuracy: 90%\n",
      "[Step 15200] Past 100 steps: Average Loss 0.479 | Accuracy: 88%\n",
      "[Step 15300] Past 100 steps: Average Loss 0.352 | Accuracy: 95%\n",
      "[Step 15400] Past 100 steps: Average Loss 0.508 | Accuracy: 86%\n",
      "[Step 15500] Past 100 steps: Average Loss 0.445 | Accuracy: 91%\n",
      "[Step 15600] Past 100 steps: Average Loss 0.326 | Accuracy: 94%\n",
      "[Step 15700] Past 100 steps: Average Loss 0.546 | Accuracy: 83%\n",
      "[Step 15800] Past 100 steps: Average Loss 0.405 | Accuracy: 91%\n",
      "[Step 15900] Past 100 steps: Average Loss 0.416 | Accuracy: 92%\n",
      "[Step 16000] Past 100 steps: Average Loss 0.467 | Accuracy: 88%\n",
      "[Step 16100] Past 100 steps: Average Loss 0.375 | Accuracy: 90%\n",
      "[Step 16200] Past 100 steps: Average Loss 0.451 | Accuracy: 87%\n",
      "[Step 16300] Past 100 steps: Average Loss 0.447 | Accuracy: 90%\n",
      "[Step 16400] Past 100 steps: Average Loss 0.420 | Accuracy: 93%\n",
      "[Step 16500] Past 100 steps: Average Loss 0.389 | Accuracy: 93%\n",
      "[Step 16600] Past 100 steps: Average Loss 0.348 | Accuracy: 96%\n",
      "[Step 16700] Past 100 steps: Average Loss 0.482 | Accuracy: 86%\n",
      "[Step 16800] Past 100 steps: Average Loss 0.426 | Accuracy: 93%\n",
      "[Step 16900] Past 100 steps: Average Loss 0.439 | Accuracy: 90%\n",
      "[Step 17000] Past 100 steps: Average Loss 0.399 | Accuracy: 92%\n",
      "[Step 17100] Past 100 steps: Average Loss 0.376 | Accuracy: 90%\n",
      "[Step 17200] Past 100 steps: Average Loss 0.496 | Accuracy: 87%\n",
      "[Step 17300] Past 100 steps: Average Loss 0.407 | Accuracy: 90%\n",
      "[Step 17400] Past 100 steps: Average Loss 0.378 | Accuracy: 92%\n",
      "[Step 17500] Past 100 steps: Average Loss 0.491 | Accuracy: 83%\n",
      "[Step 17600] Past 100 steps: Average Loss 0.388 | Accuracy: 91%\n",
      "[Step 17700] Past 100 steps: Average Loss 0.271 | Accuracy: 98%\n",
      "[Step 17800] Past 100 steps: Average Loss 0.425 | Accuracy: 93%\n",
      "[Step 17900] Past 100 steps: Average Loss 0.388 | Accuracy: 91%\n",
      "[Step 18000] Past 100 steps: Average Loss 0.336 | Accuracy: 93%\n",
      "[Step 18100] Past 100 steps: Average Loss 0.367 | Accuracy: 92%\n",
      "[Step 18200] Past 100 steps: Average Loss 0.345 | Accuracy: 93%\n",
      "[Step 18300] Past 100 steps: Average Loss 0.361 | Accuracy: 92%\n",
      "[Step 18400] Past 100 steps: Average Loss 0.352 | Accuracy: 93%\n",
      "[Step 18500] Past 100 steps: Average Loss 0.329 | Accuracy: 94%\n",
      "[Step 18600] Past 100 steps: Average Loss 0.355 | Accuracy: 93%\n",
      "[Step 18700] Past 100 steps: Average Loss 0.417 | Accuracy: 89%\n",
      "[Step 18800] Past 100 steps: Average Loss 0.308 | Accuracy: 93%\n",
      "[Step 18900] Past 100 steps: Average Loss 0.303 | Accuracy: 91%\n",
      "[Step 19000] Past 100 steps: Average Loss 0.370 | Accuracy: 90%\n",
      "[Step 19100] Past 100 steps: Average Loss 0.359 | Accuracy: 87%\n",
      "[Step 19200] Past 100 steps: Average Loss 0.333 | Accuracy: 93%\n",
      "[Step 19300] Past 100 steps: Average Loss 0.443 | Accuracy: 87%\n",
      "[Step 19400] Past 100 steps: Average Loss 0.344 | Accuracy: 95%\n",
      "[Step 19500] Past 100 steps: Average Loss 0.303 | Accuracy: 93%\n",
      "[Step 19600] Past 100 steps: Average Loss 0.364 | Accuracy: 93%\n",
      "[Step 19700] Past 100 steps: Average Loss 0.387 | Accuracy: 88%\n",
      "[Step 19800] Past 100 steps: Average Loss 0.401 | Accuracy: 88%\n",
      "[Step 19900] Past 100 steps: Average Loss 0.311 | Accuracy: 93%\n",
      "[Step 20000] Past 100 steps: Average Loss 0.305 | Accuracy: 95%\n",
      "[Step 20100] Past 100 steps: Average Loss 0.322 | Accuracy: 93%\n",
      "[Step 20200] Past 100 steps: Average Loss 0.327 | Accuracy: 92%\n",
      "[Step 20300] Past 100 steps: Average Loss 0.351 | Accuracy: 94%\n",
      "[Step 20400] Past 100 steps: Average Loss 0.286 | Accuracy: 95%\n",
      "[Step 20500] Past 100 steps: Average Loss 0.358 | Accuracy: 90%\n",
      "[Step 20600] Past 100 steps: Average Loss 0.293 | Accuracy: 96%\n",
      "[Step 20700] Past 100 steps: Average Loss 0.352 | Accuracy: 93%\n",
      "[Step 20800] Past 100 steps: Average Loss 0.319 | Accuracy: 93%\n",
      "[Step 20900] Past 100 steps: Average Loss 0.373 | Accuracy: 89%\n",
      "[Step 21000] Past 100 steps: Average Loss 0.269 | Accuracy: 94%\n",
      "[Step 21100] Past 100 steps: Average Loss 0.487 | Accuracy: 88%\n",
      "[Step 21200] Past 100 steps: Average Loss 0.359 | Accuracy: 88%\n",
      "[Step 21300] Past 100 steps: Average Loss 0.522 | Accuracy: 87%\n",
      "[Step 21400] Past 100 steps: Average Loss 0.415 | Accuracy: 89%\n",
      "[Step 21500] Past 100 steps: Average Loss 0.409 | Accuracy: 89%\n",
      "[Step 21600] Past 100 steps: Average Loss 0.350 | Accuracy: 90%\n",
      "[Step 21700] Past 100 steps: Average Loss 0.351 | Accuracy: 88%\n",
      "[Step 21800] Past 100 steps: Average Loss 0.380 | Accuracy: 91%\n",
      "[Step 21900] Past 100 steps: Average Loss 0.348 | Accuracy: 89%\n",
      "[Step 22000] Past 100 steps: Average Loss 0.408 | Accuracy: 86%\n",
      "[Step 22100] Past 100 steps: Average Loss 0.465 | Accuracy: 90%\n",
      "[Step 22200] Past 100 steps: Average Loss 0.349 | Accuracy: 94%\n",
      "[Step 22300] Past 100 steps: Average Loss 0.414 | Accuracy: 90%\n",
      "[Step 22400] Past 100 steps: Average Loss 0.561 | Accuracy: 83%\n",
      "[Step 22500] Past 100 steps: Average Loss 0.285 | Accuracy: 95%\n",
      "[Step 22600] Past 100 steps: Average Loss 0.430 | Accuracy: 89%\n",
      "[Step 22700] Past 100 steps: Average Loss 0.240 | Accuracy: 96%\n",
      "[Step 22800] Past 100 steps: Average Loss 0.310 | Accuracy: 90%\n",
      "[Step 22900] Past 100 steps: Average Loss 0.493 | Accuracy: 86%\n",
      "[Step 23000] Past 100 steps: Average Loss 0.353 | Accuracy: 92%\n",
      "[Step 23100] Past 100 steps: Average Loss 0.271 | Accuracy: 94%\n",
      "[Step 23200] Past 100 steps: Average Loss 0.376 | Accuracy: 91%\n",
      "[Step 23300] Past 100 steps: Average Loss 0.316 | Accuracy: 91%\n",
      "[Step 23400] Past 100 steps: Average Loss 0.259 | Accuracy: 93%\n",
      "[Step 23500] Past 100 steps: Average Loss 0.367 | Accuracy: 91%\n",
      "[Step 23600] Past 100 steps: Average Loss 0.299 | Accuracy: 94%\n",
      "[Step 23700] Past 100 steps: Average Loss 0.296 | Accuracy: 90%\n",
      "[Step 23800] Past 100 steps: Average Loss 0.380 | Accuracy: 90%\n",
      "[Step 23900] Past 100 steps: Average Loss 0.465 | Accuracy: 89%\n",
      "[Step 24000] Past 100 steps: Average Loss 0.426 | Accuracy: 87%\n",
      "[Step 24100] Past 100 steps: Average Loss 0.308 | Accuracy: 94%\n",
      "[Step 24200] Past 100 steps: Average Loss 0.353 | Accuracy: 93%\n",
      "[Step 24300] Past 100 steps: Average Loss 0.190 | Accuracy: 98%\n",
      "[Step 24400] Past 100 steps: Average Loss 0.348 | Accuracy: 89%\n",
      "[Step 24500] Past 100 steps: Average Loss 0.222 | Accuracy: 96%\n",
      "[Step 24600] Past 100 steps: Average Loss 0.423 | Accuracy: 91%\n",
      "[Step 24700] Past 100 steps: Average Loss 0.415 | Accuracy: 85%\n",
      "[Step 24800] Past 100 steps: Average Loss 0.218 | Accuracy: 95%\n",
      "[Step 24900] Past 100 steps: Average Loss 0.240 | Accuracy: 94%\n",
      "[Step 25000] Past 100 steps: Average Loss 0.419 | Accuracy: 88%\n",
      "[Step 25100] Past 100 steps: Average Loss 0.310 | Accuracy: 92%\n",
      "[Step 25200] Past 100 steps: Average Loss 0.195 | Accuracy: 95%\n",
      "[Step 25300] Past 100 steps: Average Loss 0.240 | Accuracy: 97%\n",
      "[Step 25400] Past 100 steps: Average Loss 0.508 | Accuracy: 83%\n",
      "[Step 25500] Past 100 steps: Average Loss 0.298 | Accuracy: 94%\n",
      "[Step 25600] Past 100 steps: Average Loss 0.472 | Accuracy: 87%\n",
      "[Step 25700] Past 100 steps: Average Loss 0.221 | Accuracy: 96%\n",
      "[Step 25800] Past 100 steps: Average Loss 0.313 | Accuracy: 88%\n",
      "[Step 25900] Past 100 steps: Average Loss 0.431 | Accuracy: 87%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 26000] Past 100 steps: Average Loss 0.295 | Accuracy: 94%\n",
      "[Step 26100] Past 100 steps: Average Loss 0.359 | Accuracy: 91%\n",
      "[Step 26200] Past 100 steps: Average Loss 0.339 | Accuracy: 89%\n",
      "[Step 26300] Past 100 steps: Average Loss 0.462 | Accuracy: 84%\n",
      "[Step 26400] Past 100 steps: Average Loss 0.352 | Accuracy: 90%\n",
      "[Step 26500] Past 100 steps: Average Loss 0.217 | Accuracy: 95%\n",
      "[Step 26600] Past 100 steps: Average Loss 0.282 | Accuracy: 91%\n",
      "[Step 26700] Past 100 steps: Average Loss 0.332 | Accuracy: 90%\n",
      "[Step 26800] Past 100 steps: Average Loss 0.216 | Accuracy: 96%\n",
      "[Step 26900] Past 100 steps: Average Loss 0.343 | Accuracy: 89%\n",
      "[Step 27000] Past 100 steps: Average Loss 0.264 | Accuracy: 94%\n",
      "[Step 27100] Past 100 steps: Average Loss 0.320 | Accuracy: 92%\n",
      "[Step 27200] Past 100 steps: Average Loss 0.318 | Accuracy: 92%\n",
      "[Step 27300] Past 100 steps: Average Loss 0.478 | Accuracy: 89%\n",
      "[Step 27400] Past 100 steps: Average Loss 0.184 | Accuracy: 98%\n",
      "[Step 27500] Past 100 steps: Average Loss 0.277 | Accuracy: 95%\n",
      "[Step 27600] Past 100 steps: Average Loss 0.257 | Accuracy: 93%\n",
      "[Step 27700] Past 100 steps: Average Loss 0.298 | Accuracy: 92%\n",
      "[Step 27800] Past 100 steps: Average Loss 0.436 | Accuracy: 89%\n",
      "[Step 27900] Past 100 steps: Average Loss 0.434 | Accuracy: 86%\n",
      "[Step 28000] Past 100 steps: Average Loss 0.412 | Accuracy: 92%\n",
      "[Step 28100] Past 100 steps: Average Loss 0.313 | Accuracy: 90%\n",
      "[Step 28200] Past 100 steps: Average Loss 0.327 | Accuracy: 91%\n",
      "[Step 28300] Past 100 steps: Average Loss 0.360 | Accuracy: 91%\n",
      "[Step 28400] Past 100 steps: Average Loss 0.357 | Accuracy: 89%\n",
      "[Step 28500] Past 100 steps: Average Loss 0.380 | Accuracy: 91%\n",
      "[Step 28600] Past 100 steps: Average Loss 0.194 | Accuracy: 96%\n",
      "[Step 28700] Past 100 steps: Average Loss 0.336 | Accuracy: 91%\n",
      "[Step 28800] Past 100 steps: Average Loss 0.273 | Accuracy: 95%\n",
      "[Step 28900] Past 100 steps: Average Loss 0.321 | Accuracy: 91%\n",
      "[Step 29000] Past 100 steps: Average Loss 0.299 | Accuracy: 96%\n",
      "[Step 29100] Past 100 steps: Average Loss 0.349 | Accuracy: 90%\n",
      "[Step 29200] Past 100 steps: Average Loss 0.422 | Accuracy: 88%\n",
      "[Step 29300] Past 100 steps: Average Loss 0.333 | Accuracy: 89%\n",
      "[Step 29400] Past 100 steps: Average Loss 0.212 | Accuracy: 94%\n",
      "[Step 29500] Past 100 steps: Average Loss 0.286 | Accuracy: 91%\n",
      "[Step 29600] Past 100 steps: Average Loss 0.257 | Accuracy: 93%\n",
      "[Step 29700] Past 100 steps: Average Loss 0.249 | Accuracy: 92%\n",
      "[Step 29800] Past 100 steps: Average Loss 0.323 | Accuracy: 91%\n",
      "[Step 29900] Past 100 steps: Average Loss 0.243 | Accuracy: 93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 33%|                                                     | 1/3 [31:21<1:02:42, 1881.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 30000] Past 100 steps: Average Loss 0.330 | Accuracy: 91%\n",
      "--- Epoch 2 ---\n",
      "[Step 100] Past 100 steps: Average Loss 0.410 | Accuracy: 91%\n",
      "[Step 200] Past 100 steps: Average Loss 0.282 | Accuracy: 91%\n",
      "[Step 300] Past 100 steps: Average Loss 0.284 | Accuracy: 91%\n",
      "[Step 400] Past 100 steps: Average Loss 0.273 | Accuracy: 95%\n",
      "[Step 500] Past 100 steps: Average Loss 0.409 | Accuracy: 87%\n",
      "[Step 600] Past 100 steps: Average Loss 0.244 | Accuracy: 92%\n",
      "[Step 700] Past 100 steps: Average Loss 0.415 | Accuracy: 89%\n",
      "[Step 800] Past 100 steps: Average Loss 0.356 | Accuracy: 88%\n",
      "[Step 900] Past 100 steps: Average Loss 0.279 | Accuracy: 95%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.385 | Accuracy: 90%\n",
      "[Step 1100] Past 100 steps: Average Loss 0.250 | Accuracy: 94%\n",
      "[Step 1200] Past 100 steps: Average Loss 0.245 | Accuracy: 96%\n",
      "[Step 1300] Past 100 steps: Average Loss 0.246 | Accuracy: 94%\n",
      "[Step 1400] Past 100 steps: Average Loss 0.342 | Accuracy: 94%\n",
      "[Step 1500] Past 100 steps: Average Loss 0.346 | Accuracy: 91%\n",
      "[Step 1600] Past 100 steps: Average Loss 0.263 | Accuracy: 94%\n",
      "[Step 1700] Past 100 steps: Average Loss 0.352 | Accuracy: 88%\n",
      "[Step 1800] Past 100 steps: Average Loss 0.196 | Accuracy: 96%\n",
      "[Step 1900] Past 100 steps: Average Loss 0.300 | Accuracy: 92%\n",
      "[Step 2000] Past 100 steps: Average Loss 0.245 | Accuracy: 95%\n",
      "[Step 2100] Past 100 steps: Average Loss 0.306 | Accuracy: 87%\n",
      "[Step 2200] Past 100 steps: Average Loss 0.557 | Accuracy: 86%\n",
      "[Step 2300] Past 100 steps: Average Loss 0.320 | Accuracy: 93%\n",
      "[Step 2400] Past 100 steps: Average Loss 0.222 | Accuracy: 94%\n",
      "[Step 2500] Past 100 steps: Average Loss 0.181 | Accuracy: 97%\n",
      "[Step 2600] Past 100 steps: Average Loss 0.327 | Accuracy: 92%\n",
      "[Step 2700] Past 100 steps: Average Loss 0.313 | Accuracy: 92%\n",
      "[Step 2800] Past 100 steps: Average Loss 0.275 | Accuracy: 93%\n",
      "[Step 2900] Past 100 steps: Average Loss 0.340 | Accuracy: 91%\n",
      "[Step 3000] Past 100 steps: Average Loss 0.197 | Accuracy: 96%\n",
      "[Step 3100] Past 100 steps: Average Loss 0.251 | Accuracy: 92%\n",
      "[Step 3200] Past 100 steps: Average Loss 0.441 | Accuracy: 91%\n",
      "[Step 3300] Past 100 steps: Average Loss 0.458 | Accuracy: 92%\n",
      "[Step 3400] Past 100 steps: Average Loss 0.340 | Accuracy: 91%\n",
      "[Step 3500] Past 100 steps: Average Loss 0.218 | Accuracy: 95%\n",
      "[Step 3600] Past 100 steps: Average Loss 0.193 | Accuracy: 97%\n",
      "[Step 3700] Past 100 steps: Average Loss 0.223 | Accuracy: 93%\n",
      "[Step 3800] Past 100 steps: Average Loss 0.277 | Accuracy: 94%\n",
      "[Step 3900] Past 100 steps: Average Loss 0.211 | Accuracy: 97%\n",
      "[Step 4000] Past 100 steps: Average Loss 0.290 | Accuracy: 91%\n",
      "[Step 4100] Past 100 steps: Average Loss 0.389 | Accuracy: 92%\n",
      "[Step 4200] Past 100 steps: Average Loss 0.440 | Accuracy: 86%\n",
      "[Step 4300] Past 100 steps: Average Loss 0.315 | Accuracy: 91%\n",
      "[Step 4400] Past 100 steps: Average Loss 0.326 | Accuracy: 90%\n",
      "[Step 4500] Past 100 steps: Average Loss 0.209 | Accuracy: 95%\n",
      "[Step 4600] Past 100 steps: Average Loss 0.263 | Accuracy: 93%\n",
      "[Step 4700] Past 100 steps: Average Loss 0.189 | Accuracy: 95%\n",
      "[Step 4800] Past 100 steps: Average Loss 0.269 | Accuracy: 91%\n",
      "[Step 4900] Past 100 steps: Average Loss 0.380 | Accuracy: 87%\n",
      "[Step 5000] Past 100 steps: Average Loss 0.212 | Accuracy: 95%\n",
      "[Step 5100] Past 100 steps: Average Loss 0.191 | Accuracy: 95%\n",
      "[Step 5200] Past 100 steps: Average Loss 0.329 | Accuracy: 90%\n",
      "[Step 5300] Past 100 steps: Average Loss 0.437 | Accuracy: 87%\n",
      "[Step 5400] Past 100 steps: Average Loss 0.167 | Accuracy: 97%\n",
      "[Step 5500] Past 100 steps: Average Loss 0.395 | Accuracy: 92%\n",
      "[Step 5600] Past 100 steps: Average Loss 0.285 | Accuracy: 92%\n",
      "[Step 5700] Past 100 steps: Average Loss 0.450 | Accuracy: 89%\n",
      "[Step 5800] Past 100 steps: Average Loss 0.345 | Accuracy: 91%\n",
      "[Step 5900] Past 100 steps: Average Loss 0.330 | Accuracy: 90%\n",
      "[Step 6000] Past 100 steps: Average Loss 0.266 | Accuracy: 91%\n",
      "[Step 6100] Past 100 steps: Average Loss 0.323 | Accuracy: 91%\n",
      "[Step 6200] Past 100 steps: Average Loss 0.262 | Accuracy: 90%\n",
      "[Step 6300] Past 100 steps: Average Loss 0.308 | Accuracy: 93%\n",
      "[Step 6400] Past 100 steps: Average Loss 0.304 | Accuracy: 92%\n",
      "[Step 6500] Past 100 steps: Average Loss 0.226 | Accuracy: 95%\n",
      "[Step 6600] Past 100 steps: Average Loss 0.223 | Accuracy: 95%\n",
      "[Step 6700] Past 100 steps: Average Loss 0.314 | Accuracy: 94%\n",
      "[Step 6800] Past 100 steps: Average Loss 0.220 | Accuracy: 95%\n",
      "[Step 6900] Past 100 steps: Average Loss 0.294 | Accuracy: 90%\n",
      "[Step 7000] Past 100 steps: Average Loss 0.345 | Accuracy: 89%\n",
      "[Step 7100] Past 100 steps: Average Loss 0.260 | Accuracy: 91%\n",
      "[Step 7200] Past 100 steps: Average Loss 0.322 | Accuracy: 92%\n",
      "[Step 7300] Past 100 steps: Average Loss 0.204 | Accuracy: 93%\n",
      "[Step 7400] Past 100 steps: Average Loss 0.263 | Accuracy: 93%\n",
      "[Step 7500] Past 100 steps: Average Loss 0.272 | Accuracy: 92%\n",
      "[Step 7600] Past 100 steps: Average Loss 0.307 | Accuracy: 91%\n",
      "[Step 7700] Past 100 steps: Average Loss 0.309 | Accuracy: 88%\n",
      "[Step 7800] Past 100 steps: Average Loss 0.489 | Accuracy: 89%\n",
      "[Step 7900] Past 100 steps: Average Loss 0.391 | Accuracy: 87%\n",
      "[Step 8000] Past 100 steps: Average Loss 0.259 | Accuracy: 94%\n",
      "[Step 8100] Past 100 steps: Average Loss 0.328 | Accuracy: 92%\n",
      "[Step 8200] Past 100 steps: Average Loss 0.330 | Accuracy: 90%\n",
      "[Step 8300] Past 100 steps: Average Loss 0.259 | Accuracy: 92%\n",
      "[Step 8400] Past 100 steps: Average Loss 0.141 | Accuracy: 98%\n",
      "[Step 8500] Past 100 steps: Average Loss 0.196 | Accuracy: 96%\n",
      "[Step 8600] Past 100 steps: Average Loss 0.366 | Accuracy: 90%\n",
      "[Step 8700] Past 100 steps: Average Loss 0.275 | Accuracy: 93%\n",
      "[Step 8800] Past 100 steps: Average Loss 0.324 | Accuracy: 86%\n",
      "[Step 8900] Past 100 steps: Average Loss 0.283 | Accuracy: 92%\n",
      "[Step 9000] Past 100 steps: Average Loss 0.330 | Accuracy: 92%\n",
      "[Step 9100] Past 100 steps: Average Loss 0.335 | Accuracy: 92%\n",
      "[Step 9200] Past 100 steps: Average Loss 0.323 | Accuracy: 93%\n",
      "[Step 9300] Past 100 steps: Average Loss 0.170 | Accuracy: 96%\n",
      "[Step 9400] Past 100 steps: Average Loss 0.215 | Accuracy: 94%\n",
      "[Step 9500] Past 100 steps: Average Loss 0.229 | Accuracy: 94%\n",
      "[Step 9600] Past 100 steps: Average Loss 0.262 | Accuracy: 93%\n",
      "[Step 9700] Past 100 steps: Average Loss 0.344 | Accuracy: 89%\n",
      "[Step 9800] Past 100 steps: Average Loss 0.222 | Accuracy: 93%\n",
      "[Step 9900] Past 100 steps: Average Loss 0.279 | Accuracy: 90%\n",
      "[Step 10000] Past 100 steps: Average Loss 0.340 | Accuracy: 92%\n",
      "[Step 10100] Past 100 steps: Average Loss 0.271 | Accuracy: 93%\n",
      "[Step 10200] Past 100 steps: Average Loss 0.259 | Accuracy: 95%\n",
      "[Step 10300] Past 100 steps: Average Loss 0.370 | Accuracy: 93%\n",
      "[Step 10400] Past 100 steps: Average Loss 0.286 | Accuracy: 92%\n",
      "[Step 10500] Past 100 steps: Average Loss 0.330 | Accuracy: 88%\n",
      "[Step 10600] Past 100 steps: Average Loss 0.264 | Accuracy: 95%\n",
      "[Step 10700] Past 100 steps: Average Loss 0.284 | Accuracy: 94%\n",
      "[Step 10800] Past 100 steps: Average Loss 0.243 | Accuracy: 95%\n",
      "[Step 10900] Past 100 steps: Average Loss 0.201 | Accuracy: 95%\n",
      "[Step 11000] Past 100 steps: Average Loss 0.272 | Accuracy: 94%\n",
      "[Step 11100] Past 100 steps: Average Loss 0.228 | Accuracy: 94%\n",
      "[Step 11200] Past 100 steps: Average Loss 0.415 | Accuracy: 88%\n",
      "[Step 11300] Past 100 steps: Average Loss 0.197 | Accuracy: 98%\n",
      "[Step 11400] Past 100 steps: Average Loss 0.252 | Accuracy: 93%\n",
      "[Step 11500] Past 100 steps: Average Loss 0.180 | Accuracy: 95%\n",
      "[Step 11600] Past 100 steps: Average Loss 0.229 | Accuracy: 95%\n",
      "[Step 11700] Past 100 steps: Average Loss 0.156 | Accuracy: 95%\n",
      "[Step 11800] Past 100 steps: Average Loss 0.392 | Accuracy: 92%\n",
      "[Step 11900] Past 100 steps: Average Loss 0.290 | Accuracy: 90%\n",
      "[Step 12000] Past 100 steps: Average Loss 0.193 | Accuracy: 92%\n",
      "[Step 12100] Past 100 steps: Average Loss 0.411 | Accuracy: 91%\n",
      "[Step 12200] Past 100 steps: Average Loss 0.306 | Accuracy: 94%\n",
      "[Step 12300] Past 100 steps: Average Loss 0.168 | Accuracy: 97%\n",
      "[Step 12400] Past 100 steps: Average Loss 0.275 | Accuracy: 92%\n",
      "[Step 12500] Past 100 steps: Average Loss 0.192 | Accuracy: 93%\n",
      "[Step 12600] Past 100 steps: Average Loss 0.277 | Accuracy: 89%\n",
      "[Step 12700] Past 100 steps: Average Loss 0.400 | Accuracy: 89%\n",
      "[Step 12800] Past 100 steps: Average Loss 0.179 | Accuracy: 98%\n",
      "[Step 12900] Past 100 steps: Average Loss 0.165 | Accuracy: 96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 13000] Past 100 steps: Average Loss 0.344 | Accuracy: 90%\n",
      "[Step 13100] Past 100 steps: Average Loss 0.380 | Accuracy: 91%\n",
      "[Step 13200] Past 100 steps: Average Loss 0.219 | Accuracy: 97%\n",
      "[Step 13300] Past 100 steps: Average Loss 0.320 | Accuracy: 90%\n",
      "[Step 13400] Past 100 steps: Average Loss 0.312 | Accuracy: 90%\n",
      "[Step 13500] Past 100 steps: Average Loss 0.318 | Accuracy: 91%\n",
      "[Step 13600] Past 100 steps: Average Loss 0.231 | Accuracy: 92%\n",
      "[Step 13700] Past 100 steps: Average Loss 0.256 | Accuracy: 92%\n",
      "[Step 13800] Past 100 steps: Average Loss 0.323 | Accuracy: 91%\n",
      "[Step 13900] Past 100 steps: Average Loss 0.464 | Accuracy: 84%\n",
      "[Step 14000] Past 100 steps: Average Loss 0.224 | Accuracy: 93%\n",
      "[Step 14100] Past 100 steps: Average Loss 0.324 | Accuracy: 89%\n",
      "[Step 14200] Past 100 steps: Average Loss 0.289 | Accuracy: 89%\n",
      "[Step 14300] Past 100 steps: Average Loss 0.342 | Accuracy: 90%\n",
      "[Step 14400] Past 100 steps: Average Loss 0.226 | Accuracy: 96%\n",
      "[Step 14500] Past 100 steps: Average Loss 0.279 | Accuracy: 94%\n",
      "[Step 14600] Past 100 steps: Average Loss 0.178 | Accuracy: 96%\n",
      "[Step 14700] Past 100 steps: Average Loss 0.286 | Accuracy: 91%\n",
      "[Step 14800] Past 100 steps: Average Loss 0.252 | Accuracy: 94%\n",
      "[Step 14900] Past 100 steps: Average Loss 0.275 | Accuracy: 94%\n",
      "[Step 15000] Past 100 steps: Average Loss 0.158 | Accuracy: 96%\n",
      "[Step 15100] Past 100 steps: Average Loss 0.286 | Accuracy: 92%\n",
      "[Step 15200] Past 100 steps: Average Loss 0.352 | Accuracy: 90%\n",
      "[Step 15300] Past 100 steps: Average Loss 0.289 | Accuracy: 93%\n",
      "[Step 15400] Past 100 steps: Average Loss 0.177 | Accuracy: 95%\n",
      "[Step 15500] Past 100 steps: Average Loss 0.309 | Accuracy: 91%\n",
      "[Step 15600] Past 100 steps: Average Loss 0.457 | Accuracy: 83%\n",
      "[Step 15700] Past 100 steps: Average Loss 0.233 | Accuracy: 94%\n",
      "[Step 15800] Past 100 steps: Average Loss 0.245 | Accuracy: 94%\n",
      "[Step 15900] Past 100 steps: Average Loss 0.485 | Accuracy: 86%\n",
      "[Step 16000] Past 100 steps: Average Loss 0.364 | Accuracy: 93%\n",
      "[Step 16100] Past 100 steps: Average Loss 0.308 | Accuracy: 91%\n",
      "[Step 16200] Past 100 steps: Average Loss 0.223 | Accuracy: 94%\n",
      "[Step 16300] Past 100 steps: Average Loss 0.440 | Accuracy: 83%\n",
      "[Step 16400] Past 100 steps: Average Loss 0.145 | Accuracy: 95%\n",
      "[Step 16500] Past 100 steps: Average Loss 0.207 | Accuracy: 93%\n",
      "[Step 16600] Past 100 steps: Average Loss 0.224 | Accuracy: 93%\n",
      "[Step 16700] Past 100 steps: Average Loss 0.314 | Accuracy: 90%\n",
      "[Step 16800] Past 100 steps: Average Loss 0.285 | Accuracy: 90%\n",
      "[Step 16900] Past 100 steps: Average Loss 0.243 | Accuracy: 93%\n",
      "[Step 17000] Past 100 steps: Average Loss 0.243 | Accuracy: 93%\n",
      "[Step 17100] Past 100 steps: Average Loss 0.318 | Accuracy: 90%\n",
      "[Step 17200] Past 100 steps: Average Loss 0.236 | Accuracy: 90%\n",
      "[Step 17300] Past 100 steps: Average Loss 0.229 | Accuracy: 93%\n",
      "[Step 17400] Past 100 steps: Average Loss 0.119 | Accuracy: 97%\n",
      "[Step 17500] Past 100 steps: Average Loss 0.345 | Accuracy: 89%\n",
      "[Step 17600] Past 100 steps: Average Loss 0.391 | Accuracy: 89%\n",
      "[Step 17700] Past 100 steps: Average Loss 0.240 | Accuracy: 93%\n",
      "[Step 17800] Past 100 steps: Average Loss 0.218 | Accuracy: 94%\n",
      "[Step 17900] Past 100 steps: Average Loss 0.302 | Accuracy: 92%\n",
      "[Step 18000] Past 100 steps: Average Loss 0.342 | Accuracy: 90%\n",
      "[Step 18100] Past 100 steps: Average Loss 0.175 | Accuracy: 97%\n",
      "[Step 18200] Past 100 steps: Average Loss 0.297 | Accuracy: 92%\n",
      "[Step 18300] Past 100 steps: Average Loss 0.298 | Accuracy: 91%\n",
      "[Step 18400] Past 100 steps: Average Loss 0.223 | Accuracy: 95%\n",
      "[Step 18500] Past 100 steps: Average Loss 0.289 | Accuracy: 91%\n",
      "[Step 18600] Past 100 steps: Average Loss 0.284 | Accuracy: 93%\n",
      "[Step 18700] Past 100 steps: Average Loss 0.248 | Accuracy: 93%\n",
      "[Step 18800] Past 100 steps: Average Loss 0.205 | Accuracy: 94%\n",
      "[Step 18900] Past 100 steps: Average Loss 0.377 | Accuracy: 96%\n",
      "[Step 19000] Past 100 steps: Average Loss 0.294 | Accuracy: 92%\n",
      "[Step 19100] Past 100 steps: Average Loss 0.173 | Accuracy: 96%\n",
      "[Step 19200] Past 100 steps: Average Loss 0.254 | Accuracy: 93%\n",
      "[Step 19300] Past 100 steps: Average Loss 0.317 | Accuracy: 91%\n",
      "[Step 19400] Past 100 steps: Average Loss 0.205 | Accuracy: 92%\n",
      "[Step 19500] Past 100 steps: Average Loss 0.448 | Accuracy: 92%\n",
      "[Step 19600] Past 100 steps: Average Loss 0.209 | Accuracy: 93%\n",
      "[Step 19700] Past 100 steps: Average Loss 0.275 | Accuracy: 93%\n",
      "[Step 19800] Past 100 steps: Average Loss 0.232 | Accuracy: 94%\n",
      "[Step 19900] Past 100 steps: Average Loss 0.213 | Accuracy: 96%\n",
      "[Step 20000] Past 100 steps: Average Loss 0.209 | Accuracy: 93%\n",
      "[Step 20100] Past 100 steps: Average Loss 0.357 | Accuracy: 86%\n",
      "[Step 20200] Past 100 steps: Average Loss 0.279 | Accuracy: 90%\n",
      "[Step 20300] Past 100 steps: Average Loss 0.323 | Accuracy: 88%\n",
      "[Step 20400] Past 100 steps: Average Loss 0.363 | Accuracy: 88%\n",
      "[Step 20500] Past 100 steps: Average Loss 0.225 | Accuracy: 92%\n",
      "[Step 20600] Past 100 steps: Average Loss 0.217 | Accuracy: 95%\n",
      "[Step 20700] Past 100 steps: Average Loss 0.143 | Accuracy: 95%\n",
      "[Step 20800] Past 100 steps: Average Loss 0.304 | Accuracy: 94%\n",
      "[Step 20900] Past 100 steps: Average Loss 0.171 | Accuracy: 96%\n",
      "[Step 21000] Past 100 steps: Average Loss 0.249 | Accuracy: 91%\n",
      "[Step 21100] Past 100 steps: Average Loss 0.366 | Accuracy: 91%\n",
      "[Step 21200] Past 100 steps: Average Loss 0.352 | Accuracy: 89%\n",
      "[Step 21300] Past 100 steps: Average Loss 0.317 | Accuracy: 90%\n",
      "[Step 21400] Past 100 steps: Average Loss 0.301 | Accuracy: 93%\n",
      "[Step 21500] Past 100 steps: Average Loss 0.463 | Accuracy: 87%\n",
      "[Step 21600] Past 100 steps: Average Loss 0.481 | Accuracy: 84%\n",
      "[Step 21700] Past 100 steps: Average Loss 0.283 | Accuracy: 93%\n",
      "[Step 21800] Past 100 steps: Average Loss 0.147 | Accuracy: 97%\n",
      "[Step 21900] Past 100 steps: Average Loss 0.326 | Accuracy: 91%\n",
      "[Step 22000] Past 100 steps: Average Loss 0.210 | Accuracy: 95%\n",
      "[Step 22100] Past 100 steps: Average Loss 0.270 | Accuracy: 93%\n",
      "[Step 22200] Past 100 steps: Average Loss 0.256 | Accuracy: 92%\n",
      "[Step 22300] Past 100 steps: Average Loss 0.157 | Accuracy: 96%\n",
      "[Step 22400] Past 100 steps: Average Loss 0.283 | Accuracy: 93%\n",
      "[Step 22500] Past 100 steps: Average Loss 0.301 | Accuracy: 92%\n",
      "[Step 22600] Past 100 steps: Average Loss 0.486 | Accuracy: 86%\n",
      "[Step 22700] Past 100 steps: Average Loss 0.264 | Accuracy: 95%\n",
      "[Step 22800] Past 100 steps: Average Loss 0.234 | Accuracy: 94%\n",
      "[Step 22900] Past 100 steps: Average Loss 0.224 | Accuracy: 92%\n",
      "[Step 23000] Past 100 steps: Average Loss 0.196 | Accuracy: 94%\n",
      "[Step 23100] Past 100 steps: Average Loss 0.242 | Accuracy: 94%\n",
      "[Step 23200] Past 100 steps: Average Loss 0.211 | Accuracy: 94%\n",
      "[Step 23300] Past 100 steps: Average Loss 0.288 | Accuracy: 92%\n",
      "[Step 23400] Past 100 steps: Average Loss 0.283 | Accuracy: 90%\n",
      "[Step 23500] Past 100 steps: Average Loss 0.235 | Accuracy: 94%\n",
      "[Step 23600] Past 100 steps: Average Loss 0.338 | Accuracy: 88%\n",
      "[Step 23700] Past 100 steps: Average Loss 0.232 | Accuracy: 93%\n",
      "[Step 23800] Past 100 steps: Average Loss 0.371 | Accuracy: 90%\n",
      "[Step 23900] Past 100 steps: Average Loss 0.220 | Accuracy: 93%\n",
      "[Step 24000] Past 100 steps: Average Loss 0.415 | Accuracy: 87%\n",
      "[Step 24100] Past 100 steps: Average Loss 0.333 | Accuracy: 91%\n",
      "[Step 24200] Past 100 steps: Average Loss 0.520 | Accuracy: 90%\n",
      "[Step 24300] Past 100 steps: Average Loss 0.310 | Accuracy: 89%\n",
      "[Step 24400] Past 100 steps: Average Loss 0.386 | Accuracy: 91%\n",
      "[Step 24500] Past 100 steps: Average Loss 0.182 | Accuracy: 95%\n",
      "[Step 24600] Past 100 steps: Average Loss 0.201 | Accuracy: 94%\n",
      "[Step 24700] Past 100 steps: Average Loss 0.365 | Accuracy: 88%\n",
      "[Step 24800] Past 100 steps: Average Loss 0.224 | Accuracy: 93%\n",
      "[Step 24900] Past 100 steps: Average Loss 0.316 | Accuracy: 91%\n",
      "[Step 25000] Past 100 steps: Average Loss 0.315 | Accuracy: 91%\n",
      "[Step 25100] Past 100 steps: Average Loss 0.484 | Accuracy: 84%\n",
      "[Step 25200] Past 100 steps: Average Loss 0.258 | Accuracy: 91%\n",
      "[Step 25300] Past 100 steps: Average Loss 0.350 | Accuracy: 92%\n",
      "[Step 25400] Past 100 steps: Average Loss 0.219 | Accuracy: 94%\n",
      "[Step 25500] Past 100 steps: Average Loss 0.265 | Accuracy: 90%\n",
      "[Step 25600] Past 100 steps: Average Loss 0.530 | Accuracy: 83%\n",
      "[Step 25700] Past 100 steps: Average Loss 0.334 | Accuracy: 88%\n",
      "[Step 25800] Past 100 steps: Average Loss 0.287 | Accuracy: 91%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 25900] Past 100 steps: Average Loss 0.318 | Accuracy: 89%\n",
      "[Step 26000] Past 100 steps: Average Loss 0.216 | Accuracy: 92%\n",
      "[Step 26100] Past 100 steps: Average Loss 0.295 | Accuracy: 90%\n",
      "[Step 26200] Past 100 steps: Average Loss 0.267 | Accuracy: 92%\n",
      "[Step 26300] Past 100 steps: Average Loss 0.205 | Accuracy: 93%\n",
      "[Step 26400] Past 100 steps: Average Loss 0.309 | Accuracy: 92%\n",
      "[Step 26500] Past 100 steps: Average Loss 0.202 | Accuracy: 97%\n",
      "[Step 26600] Past 100 steps: Average Loss 0.356 | Accuracy: 91%\n",
      "[Step 26700] Past 100 steps: Average Loss 0.273 | Accuracy: 90%\n",
      "[Step 26800] Past 100 steps: Average Loss 0.162 | Accuracy: 95%\n",
      "[Step 26900] Past 100 steps: Average Loss 0.279 | Accuracy: 90%\n",
      "[Step 27000] Past 100 steps: Average Loss 0.149 | Accuracy: 97%\n",
      "[Step 27100] Past 100 steps: Average Loss 0.247 | Accuracy: 93%\n",
      "[Step 27200] Past 100 steps: Average Loss 0.241 | Accuracy: 92%\n",
      "[Step 27300] Past 100 steps: Average Loss 0.242 | Accuracy: 93%\n",
      "[Step 27400] Past 100 steps: Average Loss 0.312 | Accuracy: 91%\n",
      "[Step 27500] Past 100 steps: Average Loss 0.438 | Accuracy: 88%\n",
      "[Step 27600] Past 100 steps: Average Loss 0.301 | Accuracy: 89%\n",
      "[Step 27700] Past 100 steps: Average Loss 0.361 | Accuracy: 92%\n",
      "[Step 27800] Past 100 steps: Average Loss 0.375 | Accuracy: 89%\n",
      "[Step 27900] Past 100 steps: Average Loss 0.235 | Accuracy: 94%\n",
      "[Step 28000] Past 100 steps: Average Loss 0.192 | Accuracy: 96%\n",
      "[Step 28100] Past 100 steps: Average Loss 0.261 | Accuracy: 93%\n",
      "[Step 28200] Past 100 steps: Average Loss 0.241 | Accuracy: 91%\n",
      "[Step 28300] Past 100 steps: Average Loss 0.224 | Accuracy: 92%\n",
      "[Step 28400] Past 100 steps: Average Loss 0.396 | Accuracy: 92%\n",
      "[Step 28500] Past 100 steps: Average Loss 0.265 | Accuracy: 92%\n",
      "[Step 28600] Past 100 steps: Average Loss 0.235 | Accuracy: 95%\n",
      "[Step 28700] Past 100 steps: Average Loss 0.330 | Accuracy: 91%\n",
      "[Step 28800] Past 100 steps: Average Loss 0.303 | Accuracy: 91%\n",
      "[Step 28900] Past 100 steps: Average Loss 0.360 | Accuracy: 90%\n",
      "[Step 29000] Past 100 steps: Average Loss 0.252 | Accuracy: 94%\n",
      "[Step 29100] Past 100 steps: Average Loss 0.211 | Accuracy: 93%\n",
      "[Step 29200] Past 100 steps: Average Loss 0.376 | Accuracy: 91%\n",
      "[Step 29300] Past 100 steps: Average Loss 0.188 | Accuracy: 93%\n",
      "[Step 29400] Past 100 steps: Average Loss 0.258 | Accuracy: 92%\n",
      "[Step 29500] Past 100 steps: Average Loss 0.409 | Accuracy: 91%\n",
      "[Step 29600] Past 100 steps: Average Loss 0.292 | Accuracy: 92%\n",
      "[Step 29700] Past 100 steps: Average Loss 0.315 | Accuracy: 91%\n",
      "[Step 29800] Past 100 steps: Average Loss 0.299 | Accuracy: 90%\n",
      "[Step 29900] Past 100 steps: Average Loss 0.235 | Accuracy: 92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 67%|                          | 2/3 [1:02:12<31:03, 1863.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 30000] Past 100 steps: Average Loss 0.251 | Accuracy: 92%\n",
      "--- Epoch 3 ---\n",
      "[Step 100] Past 100 steps: Average Loss 0.344 | Accuracy: 91%\n",
      "[Step 200] Past 100 steps: Average Loss 0.317 | Accuracy: 91%\n",
      "[Step 300] Past 100 steps: Average Loss 0.279 | Accuracy: 91%\n",
      "[Step 400] Past 100 steps: Average Loss 0.151 | Accuracy: 96%\n",
      "[Step 500] Past 100 steps: Average Loss 0.414 | Accuracy: 91%\n",
      "[Step 600] Past 100 steps: Average Loss 0.340 | Accuracy: 90%\n",
      "[Step 700] Past 100 steps: Average Loss 0.311 | Accuracy: 88%\n",
      "[Step 800] Past 100 steps: Average Loss 0.161 | Accuracy: 97%\n",
      "[Step 900] Past 100 steps: Average Loss 0.173 | Accuracy: 95%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.176 | Accuracy: 95%\n",
      "[Step 1100] Past 100 steps: Average Loss 0.298 | Accuracy: 92%\n",
      "[Step 1200] Past 100 steps: Average Loss 0.385 | Accuracy: 89%\n",
      "[Step 1300] Past 100 steps: Average Loss 0.192 | Accuracy: 95%\n",
      "[Step 1400] Past 100 steps: Average Loss 0.237 | Accuracy: 95%\n",
      "[Step 1500] Past 100 steps: Average Loss 0.446 | Accuracy: 84%\n",
      "[Step 1600] Past 100 steps: Average Loss 0.199 | Accuracy: 96%\n",
      "[Step 1700] Past 100 steps: Average Loss 0.342 | Accuracy: 92%\n",
      "[Step 1800] Past 100 steps: Average Loss 0.194 | Accuracy: 95%\n",
      "[Step 1900] Past 100 steps: Average Loss 0.221 | Accuracy: 93%\n",
      "[Step 2000] Past 100 steps: Average Loss 0.202 | Accuracy: 95%\n",
      "[Step 2100] Past 100 steps: Average Loss 0.322 | Accuracy: 91%\n",
      "[Step 2200] Past 100 steps: Average Loss 0.357 | Accuracy: 92%\n",
      "[Step 2300] Past 100 steps: Average Loss 0.279 | Accuracy: 90%\n",
      "[Step 2400] Past 100 steps: Average Loss 0.264 | Accuracy: 93%\n",
      "[Step 2500] Past 100 steps: Average Loss 0.363 | Accuracy: 89%\n",
      "[Step 2600] Past 100 steps: Average Loss 0.222 | Accuracy: 95%\n",
      "[Step 2700] Past 100 steps: Average Loss 0.375 | Accuracy: 90%\n",
      "[Step 2800] Past 100 steps: Average Loss 0.208 | Accuracy: 95%\n",
      "[Step 2900] Past 100 steps: Average Loss 0.191 | Accuracy: 97%\n",
      "[Step 3000] Past 100 steps: Average Loss 0.421 | Accuracy: 88%\n",
      "[Step 3100] Past 100 steps: Average Loss 0.281 | Accuracy: 90%\n",
      "[Step 3200] Past 100 steps: Average Loss 0.239 | Accuracy: 96%\n",
      "[Step 3300] Past 100 steps: Average Loss 0.389 | Accuracy: 88%\n",
      "[Step 3400] Past 100 steps: Average Loss 0.181 | Accuracy: 91%\n",
      "[Step 3500] Past 100 steps: Average Loss 0.278 | Accuracy: 94%\n",
      "[Step 3600] Past 100 steps: Average Loss 0.337 | Accuracy: 95%\n",
      "[Step 3700] Past 100 steps: Average Loss 0.136 | Accuracy: 96%\n",
      "[Step 3800] Past 100 steps: Average Loss 0.326 | Accuracy: 90%\n",
      "[Step 3900] Past 100 steps: Average Loss 0.151 | Accuracy: 95%\n",
      "[Step 4000] Past 100 steps: Average Loss 0.276 | Accuracy: 89%\n",
      "[Step 4100] Past 100 steps: Average Loss 0.357 | Accuracy: 88%\n",
      "[Step 4200] Past 100 steps: Average Loss 0.371 | Accuracy: 93%\n",
      "[Step 4300] Past 100 steps: Average Loss 0.267 | Accuracy: 92%\n",
      "[Step 4400] Past 100 steps: Average Loss 0.280 | Accuracy: 87%\n",
      "[Step 4500] Past 100 steps: Average Loss 0.265 | Accuracy: 89%\n",
      "[Step 4600] Past 100 steps: Average Loss 0.237 | Accuracy: 93%\n",
      "[Step 4700] Past 100 steps: Average Loss 0.287 | Accuracy: 91%\n",
      "[Step 4800] Past 100 steps: Average Loss 0.066 | Accuracy: 100%\n",
      "[Step 4900] Past 100 steps: Average Loss 0.311 | Accuracy: 91%\n",
      "[Step 5000] Past 100 steps: Average Loss 0.299 | Accuracy: 92%\n",
      "[Step 5100] Past 100 steps: Average Loss 0.358 | Accuracy: 90%\n",
      "[Step 5200] Past 100 steps: Average Loss 0.403 | Accuracy: 89%\n",
      "[Step 5300] Past 100 steps: Average Loss 0.281 | Accuracy: 93%\n",
      "[Step 5400] Past 100 steps: Average Loss 0.196 | Accuracy: 95%\n",
      "[Step 5500] Past 100 steps: Average Loss 0.160 | Accuracy: 95%\n",
      "[Step 5600] Past 100 steps: Average Loss 0.197 | Accuracy: 95%\n",
      "[Step 5700] Past 100 steps: Average Loss 0.254 | Accuracy: 91%\n",
      "[Step 5800] Past 100 steps: Average Loss 0.371 | Accuracy: 90%\n",
      "[Step 5900] Past 100 steps: Average Loss 0.252 | Accuracy: 93%\n",
      "[Step 6000] Past 100 steps: Average Loss 0.318 | Accuracy: 95%\n",
      "[Step 6100] Past 100 steps: Average Loss 0.406 | Accuracy: 89%\n",
      "[Step 6200] Past 100 steps: Average Loss 0.406 | Accuracy: 90%\n",
      "[Step 6300] Past 100 steps: Average Loss 0.356 | Accuracy: 94%\n",
      "[Step 6400] Past 100 steps: Average Loss 0.186 | Accuracy: 94%\n",
      "[Step 6500] Past 100 steps: Average Loss 0.219 | Accuracy: 94%\n",
      "[Step 6600] Past 100 steps: Average Loss 0.266 | Accuracy: 91%\n",
      "[Step 6700] Past 100 steps: Average Loss 0.316 | Accuracy: 92%\n",
      "[Step 6800] Past 100 steps: Average Loss 0.194 | Accuracy: 94%\n",
      "[Step 6900] Past 100 steps: Average Loss 0.202 | Accuracy: 94%\n",
      "[Step 7000] Past 100 steps: Average Loss 0.182 | Accuracy: 93%\n",
      "[Step 7100] Past 100 steps: Average Loss 0.197 | Accuracy: 92%\n",
      "[Step 7200] Past 100 steps: Average Loss 0.357 | Accuracy: 88%\n",
      "[Step 7300] Past 100 steps: Average Loss 0.439 | Accuracy: 91%\n",
      "[Step 7400] Past 100 steps: Average Loss 0.215 | Accuracy: 89%\n",
      "[Step 7500] Past 100 steps: Average Loss 0.316 | Accuracy: 92%\n",
      "[Step 7600] Past 100 steps: Average Loss 0.119 | Accuracy: 95%\n",
      "[Step 7700] Past 100 steps: Average Loss 0.099 | Accuracy: 97%\n",
      "[Step 7800] Past 100 steps: Average Loss 0.229 | Accuracy: 94%\n",
      "[Step 7900] Past 100 steps: Average Loss 0.255 | Accuracy: 92%\n",
      "[Step 8000] Past 100 steps: Average Loss 0.370 | Accuracy: 92%\n",
      "[Step 8100] Past 100 steps: Average Loss 0.393 | Accuracy: 90%\n",
      "[Step 8200] Past 100 steps: Average Loss 0.199 | Accuracy: 92%\n",
      "[Step 8300] Past 100 steps: Average Loss 0.333 | Accuracy: 92%\n",
      "[Step 8400] Past 100 steps: Average Loss 0.288 | Accuracy: 90%\n",
      "[Step 8500] Past 100 steps: Average Loss 0.202 | Accuracy: 94%\n",
      "[Step 8600] Past 100 steps: Average Loss 0.379 | Accuracy: 89%\n",
      "[Step 8700] Past 100 steps: Average Loss 0.184 | Accuracy: 96%\n",
      "[Step 8800] Past 100 steps: Average Loss 0.230 | Accuracy: 93%\n",
      "[Step 8900] Past 100 steps: Average Loss 0.307 | Accuracy: 93%\n",
      "[Step 9000] Past 100 steps: Average Loss 0.299 | Accuracy: 90%\n",
      "[Step 9100] Past 100 steps: Average Loss 0.164 | Accuracy: 93%\n",
      "[Step 9200] Past 100 steps: Average Loss 0.225 | Accuracy: 92%\n",
      "[Step 9300] Past 100 steps: Average Loss 0.169 | Accuracy: 93%\n",
      "[Step 9400] Past 100 steps: Average Loss 0.322 | Accuracy: 88%\n",
      "[Step 9500] Past 100 steps: Average Loss 0.403 | Accuracy: 88%\n",
      "[Step 9600] Past 100 steps: Average Loss 0.290 | Accuracy: 93%\n",
      "[Step 9700] Past 100 steps: Average Loss 0.189 | Accuracy: 95%\n",
      "[Step 9800] Past 100 steps: Average Loss 0.359 | Accuracy: 92%\n",
      "[Step 9900] Past 100 steps: Average Loss 0.223 | Accuracy: 93%\n",
      "[Step 10000] Past 100 steps: Average Loss 0.163 | Accuracy: 97%\n",
      "[Step 10100] Past 100 steps: Average Loss 0.142 | Accuracy: 96%\n",
      "[Step 10200] Past 100 steps: Average Loss 0.184 | Accuracy: 94%\n",
      "[Step 10300] Past 100 steps: Average Loss 0.160 | Accuracy: 96%\n",
      "[Step 10400] Past 100 steps: Average Loss 0.331 | Accuracy: 92%\n",
      "[Step 10500] Past 100 steps: Average Loss 0.491 | Accuracy: 92%\n",
      "[Step 10600] Past 100 steps: Average Loss 0.349 | Accuracy: 91%\n",
      "[Step 10700] Past 100 steps: Average Loss 0.307 | Accuracy: 95%\n",
      "[Step 10800] Past 100 steps: Average Loss 0.346 | Accuracy: 94%\n",
      "[Step 10900] Past 100 steps: Average Loss 0.271 | Accuracy: 92%\n",
      "[Step 11000] Past 100 steps: Average Loss 0.253 | Accuracy: 94%\n",
      "[Step 11100] Past 100 steps: Average Loss 0.357 | Accuracy: 89%\n",
      "[Step 11200] Past 100 steps: Average Loss 0.299 | Accuracy: 88%\n",
      "[Step 11300] Past 100 steps: Average Loss 0.271 | Accuracy: 90%\n",
      "[Step 11400] Past 100 steps: Average Loss 0.301 | Accuracy: 93%\n",
      "[Step 11500] Past 100 steps: Average Loss 0.204 | Accuracy: 94%\n",
      "[Step 11600] Past 100 steps: Average Loss 0.205 | Accuracy: 94%\n",
      "[Step 11700] Past 100 steps: Average Loss 0.484 | Accuracy: 87%\n",
      "[Step 11800] Past 100 steps: Average Loss 0.432 | Accuracy: 87%\n",
      "[Step 11900] Past 100 steps: Average Loss 0.275 | Accuracy: 93%\n",
      "[Step 12000] Past 100 steps: Average Loss 0.346 | Accuracy: 95%\n",
      "[Step 12100] Past 100 steps: Average Loss 0.315 | Accuracy: 90%\n",
      "[Step 12200] Past 100 steps: Average Loss 0.181 | Accuracy: 95%\n",
      "[Step 12300] Past 100 steps: Average Loss 0.207 | Accuracy: 96%\n",
      "[Step 12400] Past 100 steps: Average Loss 0.187 | Accuracy: 96%\n",
      "[Step 12500] Past 100 steps: Average Loss 0.304 | Accuracy: 92%\n",
      "[Step 12600] Past 100 steps: Average Loss 0.363 | Accuracy: 90%\n",
      "[Step 12700] Past 100 steps: Average Loss 0.299 | Accuracy: 91%\n",
      "[Step 12800] Past 100 steps: Average Loss 0.222 | Accuracy: 92%\n",
      "[Step 12900] Past 100 steps: Average Loss 0.332 | Accuracy: 92%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 13000] Past 100 steps: Average Loss 0.141 | Accuracy: 97%\n",
      "[Step 13100] Past 100 steps: Average Loss 0.257 | Accuracy: 95%\n",
      "[Step 13200] Past 100 steps: Average Loss 0.249 | Accuracy: 91%\n",
      "[Step 13300] Past 100 steps: Average Loss 0.229 | Accuracy: 94%\n",
      "[Step 13400] Past 100 steps: Average Loss 0.315 | Accuracy: 90%\n",
      "[Step 13500] Past 100 steps: Average Loss 0.381 | Accuracy: 89%\n",
      "[Step 13600] Past 100 steps: Average Loss 0.269 | Accuracy: 93%\n",
      "[Step 13700] Past 100 steps: Average Loss 0.192 | Accuracy: 96%\n",
      "[Step 13800] Past 100 steps: Average Loss 0.426 | Accuracy: 90%\n",
      "[Step 13900] Past 100 steps: Average Loss 0.271 | Accuracy: 92%\n",
      "[Step 14000] Past 100 steps: Average Loss 0.148 | Accuracy: 97%\n",
      "[Step 14100] Past 100 steps: Average Loss 0.151 | Accuracy: 96%\n",
      "[Step 14200] Past 100 steps: Average Loss 0.372 | Accuracy: 91%\n",
      "[Step 14300] Past 100 steps: Average Loss 0.262 | Accuracy: 95%\n",
      "[Step 14400] Past 100 steps: Average Loss 0.186 | Accuracy: 97%\n",
      "[Step 14500] Past 100 steps: Average Loss 0.384 | Accuracy: 92%\n",
      "[Step 14600] Past 100 steps: Average Loss 0.194 | Accuracy: 96%\n",
      "[Step 14700] Past 100 steps: Average Loss 0.288 | Accuracy: 90%\n",
      "[Step 14800] Past 100 steps: Average Loss 0.400 | Accuracy: 88%\n",
      "[Step 14900] Past 100 steps: Average Loss 0.252 | Accuracy: 92%\n",
      "[Step 15000] Past 100 steps: Average Loss 0.245 | Accuracy: 92%\n",
      "[Step 15100] Past 100 steps: Average Loss 0.190 | Accuracy: 95%\n",
      "[Step 15200] Past 100 steps: Average Loss 0.332 | Accuracy: 91%\n",
      "[Step 15300] Past 100 steps: Average Loss 0.209 | Accuracy: 95%\n",
      "[Step 15400] Past 100 steps: Average Loss 0.296 | Accuracy: 91%\n",
      "[Step 15500] Past 100 steps: Average Loss 0.397 | Accuracy: 88%\n",
      "[Step 15600] Past 100 steps: Average Loss 0.285 | Accuracy: 96%\n",
      "[Step 15700] Past 100 steps: Average Loss 0.378 | Accuracy: 87%\n",
      "[Step 15800] Past 100 steps: Average Loss 0.262 | Accuracy: 95%\n",
      "[Step 15900] Past 100 steps: Average Loss 0.284 | Accuracy: 93%\n",
      "[Step 16000] Past 100 steps: Average Loss 0.293 | Accuracy: 92%\n",
      "[Step 16100] Past 100 steps: Average Loss 0.209 | Accuracy: 94%\n",
      "[Step 16200] Past 100 steps: Average Loss 0.314 | Accuracy: 94%\n",
      "[Step 16300] Past 100 steps: Average Loss 0.265 | Accuracy: 91%\n",
      "[Step 16400] Past 100 steps: Average Loss 0.242 | Accuracy: 94%\n",
      "[Step 16500] Past 100 steps: Average Loss 0.291 | Accuracy: 94%\n",
      "[Step 16600] Past 100 steps: Average Loss 0.365 | Accuracy: 90%\n",
      "[Step 16700] Past 100 steps: Average Loss 0.350 | Accuracy: 90%\n",
      "[Step 16800] Past 100 steps: Average Loss 0.230 | Accuracy: 93%\n",
      "[Step 16900] Past 100 steps: Average Loss 0.512 | Accuracy: 91%\n",
      "[Step 17000] Past 100 steps: Average Loss 0.248 | Accuracy: 93%\n",
      "[Step 17100] Past 100 steps: Average Loss 0.280 | Accuracy: 92%\n",
      "[Step 17200] Past 100 steps: Average Loss 0.289 | Accuracy: 91%\n",
      "[Step 17300] Past 100 steps: Average Loss 0.227 | Accuracy: 93%\n",
      "[Step 17400] Past 100 steps: Average Loss 0.200 | Accuracy: 96%\n",
      "[Step 17500] Past 100 steps: Average Loss 0.216 | Accuracy: 93%\n",
      "[Step 17600] Past 100 steps: Average Loss 0.357 | Accuracy: 89%\n",
      "[Step 17700] Past 100 steps: Average Loss 0.256 | Accuracy: 94%\n",
      "[Step 17800] Past 100 steps: Average Loss 0.257 | Accuracy: 92%\n",
      "[Step 17900] Past 100 steps: Average Loss 0.258 | Accuracy: 90%\n",
      "[Step 18000] Past 100 steps: Average Loss 0.310 | Accuracy: 91%\n",
      "[Step 18100] Past 100 steps: Average Loss 0.299 | Accuracy: 93%\n",
      "[Step 18200] Past 100 steps: Average Loss 0.217 | Accuracy: 92%\n",
      "[Step 18300] Past 100 steps: Average Loss 0.201 | Accuracy: 94%\n",
      "[Step 18400] Past 100 steps: Average Loss 0.214 | Accuracy: 93%\n",
      "[Step 18500] Past 100 steps: Average Loss 0.434 | Accuracy: 85%\n",
      "[Step 18600] Past 100 steps: Average Loss 0.108 | Accuracy: 98%\n",
      "[Step 18700] Past 100 steps: Average Loss 0.316 | Accuracy: 90%\n",
      "[Step 18800] Past 100 steps: Average Loss 0.259 | Accuracy: 94%\n",
      "[Step 18900] Past 100 steps: Average Loss 0.234 | Accuracy: 90%\n",
      "[Step 19000] Past 100 steps: Average Loss 0.271 | Accuracy: 93%\n",
      "[Step 19100] Past 100 steps: Average Loss 0.273 | Accuracy: 91%\n",
      "[Step 19200] Past 100 steps: Average Loss 0.171 | Accuracy: 95%\n",
      "[Step 19300] Past 100 steps: Average Loss 0.115 | Accuracy: 97%\n",
      "[Step 19400] Past 100 steps: Average Loss 0.123 | Accuracy: 97%\n",
      "[Step 19500] Past 100 steps: Average Loss 0.260 | Accuracy: 93%\n",
      "[Step 19600] Past 100 steps: Average Loss 0.308 | Accuracy: 90%\n",
      "[Step 19700] Past 100 steps: Average Loss 0.259 | Accuracy: 94%\n",
      "[Step 19800] Past 100 steps: Average Loss 0.270 | Accuracy: 93%\n",
      "[Step 19900] Past 100 steps: Average Loss 0.319 | Accuracy: 90%\n",
      "[Step 20000] Past 100 steps: Average Loss 0.327 | Accuracy: 91%\n",
      "[Step 20100] Past 100 steps: Average Loss 0.363 | Accuracy: 91%\n",
      "[Step 20200] Past 100 steps: Average Loss 0.493 | Accuracy: 88%\n",
      "[Step 20300] Past 100 steps: Average Loss 0.379 | Accuracy: 87%\n",
      "[Step 20400] Past 100 steps: Average Loss 0.192 | Accuracy: 94%\n",
      "[Step 20500] Past 100 steps: Average Loss 0.280 | Accuracy: 96%\n",
      "[Step 20600] Past 100 steps: Average Loss 0.250 | Accuracy: 93%\n",
      "[Step 20700] Past 100 steps: Average Loss 0.247 | Accuracy: 96%\n",
      "[Step 20800] Past 100 steps: Average Loss 0.442 | Accuracy: 88%\n",
      "[Step 20900] Past 100 steps: Average Loss 0.222 | Accuracy: 96%\n",
      "[Step 21000] Past 100 steps: Average Loss 0.103 | Accuracy: 97%\n",
      "[Step 21100] Past 100 steps: Average Loss 0.226 | Accuracy: 93%\n",
      "[Step 21200] Past 100 steps: Average Loss 0.263 | Accuracy: 92%\n",
      "[Step 21300] Past 100 steps: Average Loss 0.399 | Accuracy: 94%\n",
      "[Step 21400] Past 100 steps: Average Loss 0.274 | Accuracy: 91%\n",
      "[Step 21500] Past 100 steps: Average Loss 0.341 | Accuracy: 93%\n",
      "[Step 21600] Past 100 steps: Average Loss 0.282 | Accuracy: 94%\n",
      "[Step 21700] Past 100 steps: Average Loss 0.183 | Accuracy: 95%\n",
      "[Step 21800] Past 100 steps: Average Loss 0.496 | Accuracy: 89%\n",
      "[Step 21900] Past 100 steps: Average Loss 0.341 | Accuracy: 90%\n",
      "[Step 22000] Past 100 steps: Average Loss 0.296 | Accuracy: 92%\n",
      "[Step 22100] Past 100 steps: Average Loss 0.249 | Accuracy: 94%\n",
      "[Step 22200] Past 100 steps: Average Loss 0.384 | Accuracy: 89%\n",
      "[Step 22300] Past 100 steps: Average Loss 0.226 | Accuracy: 93%\n",
      "[Step 22400] Past 100 steps: Average Loss 0.199 | Accuracy: 93%\n",
      "[Step 22500] Past 100 steps: Average Loss 0.349 | Accuracy: 89%\n",
      "[Step 22600] Past 100 steps: Average Loss 0.254 | Accuracy: 92%\n",
      "[Step 22700] Past 100 steps: Average Loss 0.131 | Accuracy: 96%\n",
      "[Step 22800] Past 100 steps: Average Loss 0.361 | Accuracy: 90%\n",
      "[Step 22900] Past 100 steps: Average Loss 0.374 | Accuracy: 86%\n",
      "[Step 23000] Past 100 steps: Average Loss 0.291 | Accuracy: 92%\n",
      "[Step 23100] Past 100 steps: Average Loss 0.286 | Accuracy: 96%\n",
      "[Step 23200] Past 100 steps: Average Loss 0.227 | Accuracy: 94%\n",
      "[Step 23300] Past 100 steps: Average Loss 0.237 | Accuracy: 92%\n",
      "[Step 23400] Past 100 steps: Average Loss 0.422 | Accuracy: 91%\n",
      "[Step 23500] Past 100 steps: Average Loss 0.365 | Accuracy: 91%\n",
      "[Step 23600] Past 100 steps: Average Loss 0.177 | Accuracy: 94%\n",
      "[Step 23700] Past 100 steps: Average Loss 0.325 | Accuracy: 90%\n",
      "[Step 23800] Past 100 steps: Average Loss 0.232 | Accuracy: 94%\n",
      "[Step 23900] Past 100 steps: Average Loss 0.206 | Accuracy: 94%\n",
      "[Step 24000] Past 100 steps: Average Loss 0.548 | Accuracy: 83%\n",
      "[Step 24100] Past 100 steps: Average Loss 0.263 | Accuracy: 92%\n",
      "[Step 24200] Past 100 steps: Average Loss 0.416 | Accuracy: 86%\n",
      "[Step 24300] Past 100 steps: Average Loss 0.328 | Accuracy: 90%\n",
      "[Step 24400] Past 100 steps: Average Loss 0.205 | Accuracy: 90%\n",
      "[Step 24500] Past 100 steps: Average Loss 0.243 | Accuracy: 93%\n",
      "[Step 24600] Past 100 steps: Average Loss 0.186 | Accuracy: 92%\n",
      "[Step 24700] Past 100 steps: Average Loss 0.163 | Accuracy: 92%\n",
      "[Step 24800] Past 100 steps: Average Loss 0.260 | Accuracy: 91%\n",
      "[Step 24900] Past 100 steps: Average Loss 0.126 | Accuracy: 98%\n",
      "[Step 25000] Past 100 steps: Average Loss 0.253 | Accuracy: 90%\n",
      "[Step 25100] Past 100 steps: Average Loss 0.340 | Accuracy: 88%\n",
      "[Step 25200] Past 100 steps: Average Loss 0.201 | Accuracy: 95%\n",
      "[Step 25300] Past 100 steps: Average Loss 0.378 | Accuracy: 89%\n",
      "[Step 25400] Past 100 steps: Average Loss 0.203 | Accuracy: 94%\n",
      "[Step 25500] Past 100 steps: Average Loss 0.231 | Accuracy: 93%\n",
      "[Step 25600] Past 100 steps: Average Loss 0.361 | Accuracy: 90%\n",
      "[Step 25700] Past 100 steps: Average Loss 0.363 | Accuracy: 91%\n",
      "[Step 25800] Past 100 steps: Average Loss 0.282 | Accuracy: 93%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 25900] Past 100 steps: Average Loss 0.296 | Accuracy: 94%\n",
      "[Step 26000] Past 100 steps: Average Loss 0.248 | Accuracy: 90%\n",
      "[Step 26100] Past 100 steps: Average Loss 0.317 | Accuracy: 92%\n",
      "[Step 26200] Past 100 steps: Average Loss 0.255 | Accuracy: 92%\n",
      "[Step 26300] Past 100 steps: Average Loss 0.310 | Accuracy: 91%\n",
      "[Step 26400] Past 100 steps: Average Loss 0.411 | Accuracy: 90%\n",
      "[Step 26500] Past 100 steps: Average Loss 0.241 | Accuracy: 91%\n",
      "[Step 26600] Past 100 steps: Average Loss 0.200 | Accuracy: 93%\n",
      "[Step 26700] Past 100 steps: Average Loss 0.517 | Accuracy: 91%\n",
      "[Step 26800] Past 100 steps: Average Loss 0.196 | Accuracy: 94%\n",
      "[Step 26900] Past 100 steps: Average Loss 0.205 | Accuracy: 95%\n",
      "[Step 27000] Past 100 steps: Average Loss 0.249 | Accuracy: 94%\n",
      "[Step 27100] Past 100 steps: Average Loss 0.239 | Accuracy: 92%\n",
      "[Step 27200] Past 100 steps: Average Loss 0.191 | Accuracy: 95%\n",
      "[Step 27300] Past 100 steps: Average Loss 0.278 | Accuracy: 92%\n",
      "[Step 27400] Past 100 steps: Average Loss 0.154 | Accuracy: 96%\n",
      "[Step 27500] Past 100 steps: Average Loss 0.268 | Accuracy: 89%\n",
      "[Step 27600] Past 100 steps: Average Loss 0.359 | Accuracy: 90%\n",
      "[Step 27700] Past 100 steps: Average Loss 0.318 | Accuracy: 89%\n",
      "[Step 27800] Past 100 steps: Average Loss 0.415 | Accuracy: 88%\n",
      "[Step 27900] Past 100 steps: Average Loss 0.204 | Accuracy: 94%\n",
      "[Step 28000] Past 100 steps: Average Loss 0.367 | Accuracy: 86%\n",
      "[Step 28100] Past 100 steps: Average Loss 0.319 | Accuracy: 91%\n",
      "[Step 28200] Past 100 steps: Average Loss 0.336 | Accuracy: 92%\n",
      "[Step 28300] Past 100 steps: Average Loss 0.356 | Accuracy: 88%\n",
      "[Step 28400] Past 100 steps: Average Loss 0.331 | Accuracy: 90%\n",
      "[Step 28500] Past 100 steps: Average Loss 0.200 | Accuracy: 95%\n",
      "[Step 28600] Past 100 steps: Average Loss 0.304 | Accuracy: 91%\n",
      "[Step 28700] Past 100 steps: Average Loss 0.264 | Accuracy: 92%\n",
      "[Step 28800] Past 100 steps: Average Loss 0.317 | Accuracy: 95%\n",
      "[Step 28900] Past 100 steps: Average Loss 0.284 | Accuracy: 91%\n",
      "[Step 29000] Past 100 steps: Average Loss 0.235 | Accuracy: 93%\n",
      "[Step 29100] Past 100 steps: Average Loss 0.177 | Accuracy: 93%\n",
      "[Step 29200] Past 100 steps: Average Loss 0.402 | Accuracy: 86%\n",
      "[Step 29300] Past 100 steps: Average Loss 0.156 | Accuracy: 95%\n",
      "[Step 29400] Past 100 steps: Average Loss 0.227 | Accuracy: 91%\n",
      "[Step 29500] Past 100 steps: Average Loss 0.367 | Accuracy: 90%\n",
      "[Step 29600] Past 100 steps: Average Loss 0.195 | Accuracy: 96%\n",
      "[Step 29700] Past 100 steps: Average Loss 0.219 | Accuracy: 94%\n",
      "[Step 29800] Past 100 steps: Average Loss 0.227 | Accuracy: 95%\n",
      "[Step 29900] Past 100 steps: Average Loss 0.260 | Accuracy: 94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [1:33:48<00:00, 1876.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 30000] Past 100 steps: Average Loss 0.416 | Accuracy: 90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Begin Training !!!')\n",
    "\n",
    "# Using 3 epochs to train\n",
    "for epoch in tqdm(range(3)):\n",
    "    print('--- Epoch %d ---' % (epoch + 1))\n",
    "\n",
    "    # Shuffle the training data\n",
    "    permutation = np.random.permutation(len(train_images))\n",
    "    train_images = train_images[permutation]\n",
    "    train_labels = train_labels[permutation]\n",
    "\n",
    "    # Training\n",
    "    loss = 0\n",
    "    num_of_correct = 0\n",
    "    \n",
    "    for i, (im, label) in enumerate(zip(train_images, train_labels)):\n",
    "        if i % 100 == 99:\n",
    "            print(\n",
    "                '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n",
    "                (i + 1, loss / 100, num_of_correct)\n",
    "            )\n",
    "            loss = 0\n",
    "            num_of_correct = 0\n",
    "\n",
    "        l, accuracy = train(im, label)\n",
    "        loss += l\n",
    "        num_of_correct += accuracy\n",
    "        \n",
    "    loss_stats['train'].append(loss/len(train_images))\n",
    "    accuracy_stats['train'].append(num_of_correct/len(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f921fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b6c562c",
   "metadata": {},
   "source": [
    "### Testing the Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aac9b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.30123210401327255\n",
      "Test Accuracy: 0.919\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "num_of_correct = 0\n",
    "num_of_tests = len(test_images)\n",
    "\n",
    "for im, label in zip(test_images, test_labels):\n",
    "    _, l, accuracy = forward(im, label)\n",
    "    loss += l\n",
    "    num_of_correct += accuracy\n",
    "\n",
    "print('Test Loss:', loss / num_of_tests)\n",
    "print('Test Accuracy:', num_of_correct / num_of_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37ce13c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9190\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(num_of_correct)\n",
    "print(num_of_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f760bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict the values from the testing dataset\n",
    "pred = model.predict(test_images)\n",
    "# Convert predictions classes to one hot vectors \n",
    "pred_classes = np.argmax(pred,axis = 1) \n",
    "# Convert testing observations to one hot vectors\n",
    "true = np.argmax(test_labels, axis = 1)\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = tf.math.confusion_matrix(true, pred_classes) \n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec5317e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9cc0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ccb4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36a31930",
   "metadata": {},
   "source": [
    "### Tensorflow Model for MNIST dataset with same parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a94c7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Conv2D, Dense, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9360a105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "425455aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images[:30000]\n",
    "train_labels = train_labels[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ee15b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], train_images.shape[1], train_images.shape[2], 1)\n",
    "train_images = train_images / 255\n",
    "test_images = test_images.reshape(test_images.shape[0], test_images.shape[1], test_images.shape[2], 1)\n",
    "test_images = test_images/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0522867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.one_hot(train_labels.astype(np.int32), depth=10)\n",
    "test_labels = tf.one_hot(test_labels.astype(np.int32), depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7923ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc83b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='valid', activation='relu', input_shape=input_shape),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(epsilon=1e-03), loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "404ead23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.6222 - acc: 0.8392\n",
      "Epoch 2/3\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.3305 - acc: 0.9042\n",
      "Epoch 3/3\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.3005 - acc: 0.9138\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "caf9ec4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2777 - acc: 0.9190\n",
      "Test Loss: 0.28\n",
      "Test Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test Loss:', np.round(test_loss, 2))\n",
    "print('Test Accuracy:', np.round(test_acc, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20927504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "env_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
